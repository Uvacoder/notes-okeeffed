{"version":3,"sources":["webpack:///../manual/Machine-Learning/Machine-Learning-Intro.md"],"names":["_frontmatter","layoutProps","MDXLayout","DefaultLayout","MDXContent","components","props","mdxType","parentName","isMDXComponent"],"mappings":"ofAMO,IAAMA,EAAe,Q,mOAE5B,IAKMC,EAAc,CAClBD,gBAEIE,EAAYC,IACH,SAASC,EAAT,GAGZ,IAFDC,EAEC,EAFDA,WACGC,E,oIACF,mBACD,OAAO,YAACJ,EAAD,KAAeD,EAAiBK,EAAhC,CAAuCD,WAAYA,EAAYE,QAAQ,cAG5E,iBAAQ,CACN,GAAM,6BADR,6BAGA,iBAAQ,CACN,GAAM,qBADR,qBAMA,sBACE,kBAAIC,WAAW,MAAK,mBAAGA,WAAW,MAAS,CACvC,KAAQ,+BADQ,6BAEkB,kBAAIA,WAAW,MACjD,kBAAIA,WAAW,MAAK,mBAAGA,WAAW,MAAS,CACvC,KAAQ,uBADQ,sBAGpB,kBAAIA,WAAW,MAAK,mBAAGA,WAAW,MAAS,CACvC,KAAQ,8BADQ,6BAEkB,kBAAIA,WAAW,MACjD,kBAAIA,WAAW,MAAK,mBAAGA,WAAW,MAAS,CACvC,KAAQ,iCADQ,kCAIxB,kBAAIA,WAAW,MAAK,mBAAGA,WAAW,MAAS,CACvC,KAAQ,0CADQ,0CAE+B,kBAAIA,WAAW,MAC9D,kBAAIA,WAAW,MAAK,mBAAGA,WAAW,MAAS,CACvC,KAAQ,gDADQ,gDAGpB,kBAAIA,WAAW,MAAK,mBAAGA,WAAW,MAAS,CACvC,KAAQ,4CADQ,4CAGpB,kBAAIA,WAAW,MAAK,mBAAGA,WAAW,MAAS,CACvC,KAAQ,yDADQ,yDAGpB,kBAAIA,WAAW,MAAK,mBAAGA,WAAW,MAAS,CACvC,KAAQ,qCADQ,qCAGpB,kBAAIA,WAAW,MAAK,mBAAGA,WAAW,MAAS,CACvC,KAAQ,qDADQ,oDAGpB,kBAAIA,WAAW,MAAK,mBAAGA,WAAW,MAAS,CACvC,KAAQ,uDADQ,wDAIxB,kBAAIA,WAAW,MAAK,mBAAGA,WAAW,MAAS,CACvC,KAAQ,0BADQ,wBAEa,kBAAIA,WAAW,MAC5C,kBAAIA,WAAW,MAAK,mBAAGA,WAAW,MAAS,CACvC,KAAQ,2BADQ,0BAGpB,kBAAIA,WAAW,MAAK,mBAAGA,WAAW,MAAS,CACvC,KAAQ,2CADQ,0CAGpB,kBAAIA,WAAW,MAAK,mBAAGA,WAAW,MAAS,CACvC,KAAQ,qCADQ,oCAGpB,kBAAIA,WAAW,MAAK,mBAAGA,WAAW,MAAS,CACvC,KAAQ,oCADQ,mCAGpB,kBAAIA,WAAW,MAAK,mBAAGA,WAAW,MAAS,CACvC,KAAQ,yBADQ,wBAGpB,kBAAIA,WAAW,MAAK,mBAAGA,WAAW,MAAS,CACvC,KAAQ,iCADQ,gCAGpB,kBAAIA,WAAW,MAAK,mBAAGA,WAAW,MAAS,CACvC,KAAQ,4BADQ,2BAGpB,kBAAIA,WAAW,MAAK,mBAAGA,WAAW,MAAS,CACvC,KAAQ,+BADQ,gCAIxB,kBAAIA,WAAW,MAAK,mBAAGA,WAAW,MAAS,CACvC,KAAQ,oBADQ,kBAEO,kBAAIA,WAAW,MACtC,kBAAIA,WAAW,MAAK,mBAAGA,WAAW,MAAS,CACvC,KAAQ,gCADQ,+BAGpB,kBAAIA,WAAW,MAAK,mBAAGA,WAAW,MAAS,CACvC,KAAQ,0CADQ,yCAGpB,kBAAIA,WAAW,MAAK,mBAAGA,WAAW,MAAS,CACvC,KAAQ,2BADQ,0BAGpB,kBAAIA,WAAW,MAAK,mBAAGA,WAAW,MAAS,CACvC,KAAQ,yBADQ,wBAGpB,kBAAIA,WAAW,MAAK,mBAAGA,WAAW,MAAS,CACvC,KAAQ,8BADQ,6BAGpB,kBAAIA,WAAW,MAAK,mBAAGA,WAAW,MAAS,CACvC,KAAQ,0BADQ,yBAGpB,kBAAIA,WAAW,MAAK,mBAAGA,WAAW,MAAS,CACvC,KAAQ,yCADQ,6CAS9B,uBACA,iBAAQ,CACN,GAAM,4BADR,6BAGA,6EACA,oHACA,uEACA,sIACA,+HACA,iFACA,uBAAK,sBAAMA,WAAW,OAAU,IAA3B,iFAML,oEACA,wFACA,qBAAG,kBAAIA,WAAW,KAAf,gBACH,iCAAgB,kBAAIA,WAAW,KAAf,sBAAhB,cACA,uBAAK,sBAAMA,WAAW,OAAU,IAA3B,ihEA6CL,iBAAQ,CACN,GAAM,0BADR,0BAGA,qKACA,waACA,yhBACA,uBAAK,sBAAMA,WAAW,OAAU,IAA3B,6rCA0BL,uBACA,iBAAQ,CACN,GAAM,wCADR,0CAGA,yEACA,qBAAG,sBAAQA,WAAW,KAAnB,mBACH,4EACA,qBAAG,0BAAYA,WAAW,KAAvB,wBAAH,OAA2E,kBAAIA,WAAW,KAAf,YAA3E,OAAuH,0BAAYA,WAAW,KAAvB,eACvH,qBAAG,0BAAYA,WAAW,KAAvB,eAAH,OAAkE,kBAAIA,WAAW,KAAf,cAAlE,OAAgH,0BAAYA,WAAW,KAAvB,UAChH,2EACA,0EACA,qBAAG,sBAAQA,WAAW,KAAnB,eACH,oGACA,sIACA,qBAAG,0BAAYA,WAAW,KAAvB,cAAH,OAAiE,kBAAIA,WAAW,KAAf,uBAAjE,OAAwH,0BAAYA,WAAW,KAAvB,aACxH,qIACA,2FACA,qBAAG,sBAAQA,WAAW,KAAnB,eACH,kDAAiC,0BAAYA,WAAW,KAAvB,WAAjC,oBAAyG,0BAAYA,WAAW,KAAvB,cAAzG,uGACA,2DACA,8BAAa,0BAAYA,WAAW,KAAvB,qEACb,iGACA,iBAAQ,CACN,GAAM,yCADR,0CAGA,ocACA,2jBACA,sJACA,uBAAK,sBAAMA,WAAW,OAAU,IAA3B,4vBAuBL,iBAAQ,CACN,GAAM,qCADR,sCAGA,6YACA,uPACA,uBAAK,sBAAMA,WAAW,OAAU,IAA3B,4gBAgBL,iBAAQ,CACN,GAAM,kDADR,mDAGA,yUACA,0QACA,iNACA,2QACA,uBAAK,sBAAMA,WAAW,OAAU,IAA3B,glBAqBL,iBAAQ,CACN,GAAM,8BADR,+BAGA,2EACA,kCAAiB,0BAAYA,WAAW,KAAvB,QAAjB,mFAAqJ,0BAAYA,WAAW,KAAvB,SAArJ,gDAAuP,0BAAYA,WAAW,KAAvB,uBAAvP,KACA,qBAAG,0BAAYA,WAAW,KAAvB,aAAH,gGAAyJ,0BAAYA,WAAW,KAAvB,yBAAzJ,iHACA,qBAAG,sBAAQA,WAAW,KAAnB,6BACH,sBACE,kBAAIA,WAAW,MAAf,yBAA8C,0BAAYA,WAAW,MAAvB,WAA9C,qBAAwH,0BAAYA,WAAW,MAAvB,aAAxH,WACA,kBAAIA,WAAW,MAAf,gJACA,kBAAIA,WAAW,MAAf,uNAA4O,0BAAYA,WAAW,MAAvB,yBAE9O,iBAAQ,CACN,GAAM,8CADR,8CAGA,gUACA,oFACA,8MACA,kTACA,6EACA,uBAAK,sBAAMA,WAAW,OAAU,IAA3B,spDAwCL,iBAAQ,CACN,GAAM,gDADR,gDAGA,uBAAK,sBAAMA,WAAW,OAAU,IAA3B,m4HAsFL,8RACA,oNACA,uBAAK,sBAAMA,WAAW,OAAU,IAA3B,+gBAoBL,uBACA,iBAAQ,CACN,GAAM,wBADR,wBAGA,8GACA,sBACE,kBAAIA,WAAW,MAAf,YACA,kBAAIA,WAAW,MAAf,oBACA,kBAAIA,WAAW,MAAf,qBAEF,qBAAG,sBAAQA,WAAW,KAAnB,2BACH,iGACA,qBAAG,0BAAYA,WAAW,KAAvB,6BACH,gIACA,mGACA,qBAAG,kBAAIA,WAAW,KAAf,uBACH,uFACA,wGACA,qLACA,qDAAoC,0BAAYA,WAAW,KAAvB,2BAApC,QAAgH,0BAAYA,WAAW,KAAvB,wBAAhH,oFACA,0DAAyC,0BAAYA,WAAW,KAAvB,4BAAzC,KACA,2CAA0B,0BAAYA,WAAW,KAAvB,sBAA1B,8EAAuK,0BAAYA,WAAW,KAAvB,aAAvK,KACA,qBAAG,sBAAQA,WAAW,KAAnB,uBACH,uDACA,qBAAG,sBAAQA,WAAW,KAAnB,uBACH,4GACA,oEACA,sBACE,kBAAIA,WAAW,MAAf,4DACA,kBAAIA,WAAW,MAAf,yDAEF,uDACA,yCACA,sBACE,kBAAIA,WAAW,MAAf,8BACA,kBAAIA,WAAW,MAAf,YACA,kBAAIA,WAAW,MAAf,aAEF,0CACA,sBACE,kBAAIA,WAAW,MAAf,wCACA,kBAAIA,WAAW,MAAf,yBACA,kBAAIA,WAAW,MAAf,aAEF,2EAA0D,0BAAYA,WAAW,KAAvB,mDAC1D,iBAAQ,CACN,GAAM,oBADR,oBAGA,0XACA,gSACA,8LACA,iNACA,uBAAK,sBAAMA,WAAW,OAAU,IAA3B,o1BA0BL,gIACA,kVAIA,uBAAK,sBAAMA,WAAW,OAAU,IAA3B,skBA4BL,iBAAQ,CACN,GAAM,oCADR,oCAGA,uBAAK,sBAAMA,WAAW,OAAU,IAA3B,q1BAyBL,kCAAiB,0BAAYA,WAAW,KAAvB,QAAjB,8CACA,uBAAK,sBAAMA,WAAW,OAAU,IAA3B,6fAoBL,mQACA,iBAAQ,CACN,GAAM,8BADR,8BAGA,oTACA,oLACA,oEACA,sBACE,kBAAIA,WAAW,MACb,iBAAGA,WAAW,MAAd,+GAEF,kBAAIA,WAAW,MACb,iBAAGA,WAAW,MAAd,sJAGJ,uBAAK,sBAAMA,WAAW,OAAU,IAA3B,u4BA0BL,uZACA,iBAAQ,CACN,GAAM,6BADR,6BAGA,sHACA,uGACA,6FACA,sDAAqC,0BAAYA,WAAW,KAAvB,OAArC,yCAA8H,0BAAYA,WAAW,KAAvB,YAA9H,gDAAmO,0BAAYA,WAAW,KAAvB,YAAnO,oCAA4T,0BAAYA,WAAW,KAAvB,UAA5T,mCACA,qBAAG,sBAAQA,WAAW,KAAnB,sBACH,6GACA,mFACA,gFACA,qBAAG,sBAAQA,WAAW,KAAnB,yBACH,6JACA,qBAAG,sBAAQA,WAAW,KAAnB,2BACH,4BAAW,0BAAYA,WAAW,KAAvB,gBAAX,8BAAkG,0BAAYA,WAAW,KAAvB,YAAlG,oJAA2S,0BAAYA,WAAW,KAAvB,YAA3S,qBACA,wJACA,qBAAG,kBAAIA,WAAW,KAAf,iCAAH,6EACA,qBAAG,kBAAIA,WAAW,KAAf,sBAAH,oDAAsG,0BAAYA,WAAW,KAAvB,cAAtG,2BAAwL,0BAAYA,WAAW,KAAvB,oBAAxL,KACA,qBAAG,kBAAIA,WAAW,KAAf,oBAAH,iLACA,iBAAQ,CACN,GAAM,kBADR,kBAGA,yVACA,sRACA,mLACA,uBAAK,sBAAMA,WAAW,OAAU,IAA3B,2MAOL,uBAAK,sBAAMA,WAAW,OAAU,IAA3B,siCA6BL,iTACA,wSACA,uBAAK,sBAAMA,WAAW,OAAU,IAA3B,wrBA2BL,iBAAQ,CACN,GAAM,0BADR,0BAGA,gTACA,gHACA,uBAAK,sBAAMA,WAAW,OAAU,IAA3B,29BAmCL,8JACA,iBAAQ,CACN,GAAM,qBADR,qBAGA,gEACA,+BAAc,0BAAYA,WAAW,KAAvB,QAAd,QAAuE,0BAAYA,WAAW,KAAvB,YAAvE,iBACA,mDAAkC,0BAAYA,WAAW,KAAvB,cAAlC,SAAkG,0BAAYA,WAAW,KAAvB,oBAAlG,0BAAyL,0BAAYA,WAAW,KAAvB,mBAAzL,YAAiQ,0BAAYA,WAAW,KAAvB,qBAAjQ,KACA,8HAEA,qBAAG,kBAAIA,WAAW,KAAf,eACH,uIACsE,0BAAYA,WAAW,KAAvB,uBACtE,kNACA,qBAAG,kBAAIA,WAAW,KAAf,mBACH,gFAA+D,0BAAYA,WAAW,KAAvB,gBAA/D,yDAEA,iDACA,wKACA,qBAAG,kBAAIA,WAAW,KAAf,2BACH,uBAAK,sBAAMA,WAAW,OAAU,IAA3B,yDAGL,qBAAG,sBAAQA,WAAW,KAAnB,iCACH,qBAAG,0BAAYA,WAAW,KAAvB,YAAH,2FAEA,yFACA,qBAAG,0BAAYA,WAAW,KAAvB,8BACH,sEACA,oEACA,mJAGA,qGACA,2BAAU,0BAAYA,WAAW,KAAvB,YAAV,iEAAgI,0BAAYA,WAAW,KAAvB,eAAhI,KACA,iBAAQ,CACN,GAAM,wBADR,wBAGA,uBAAK,sBAAMA,WAAW,OAAU,IAA3B,60BAyBL,mPACA,+MACA,iNACA,iRACA,kPACA,uBAAK,sBAAMA,WAAW,OAAU,IAA3B,o1BAyBL,8TACA,uBACA,iBAAQ,CACN,GAAM,kBADR,kBAGA,qFACA,qBAAG,0BAAYA,WAAW,KAAvB,eAAH,uCACA,mHACA,qBAAG,0BAAYA,WAAW,KAAvB,yBAAH,kBACA,qBAAG,0BAAYA,WAAW,KAAvB,6BAAH,4BACA,qBAAG,sBAAQA,WAAW,KAAnB,YACH,sBACE,kBAAIA,WAAW,MAAf,mCACA,kBAAIA,WAAW,MAAf,oCACA,kBAAIA,WAAW,MAAf,iFACA,kBAAIA,WAAW,MAAf,4CACA,kBAAIA,WAAW,MAAf,+CAEF,qBAAG,sBAAQA,WAAW,KAAnB,mBACH,iFACA,2EACA,sJAGA,mEACA,8DACA,uFACA,+DACA,yFACA,qBAAG,sBAAQA,WAAW,KAAnB,wBACH,sBACE,kBAAIA,WAAW,MAAf,mCACA,kBAAIA,WAAW,MAAf,0CAEF,qBAAG,sBAAQA,WAAW,KAAnB,iBACH,sBACE,kBAAIA,WAAW,MAAf,sBACA,kBAAIA,WAAW,MAAf,qDACA,kBAAIA,WAAW,MAAf,6EACA,kBAAIA,WAAW,MAAf,gEAEF,qBAAG,sBAAQA,WAAW,KAAnB,4BACH,sBACE,kBAAIA,WAAW,MAAf,iEACA,kBAAIA,WAAW,MAAf,2DACA,kBAAIA,WAAW,MAAf,yGAA8H,0BAAYA,WAAW,MAAvB,oBAA9H,eAEF,qBAAG,sBAAQA,WAAW,KAAnB,qBACH,2IACA,wJAEA,wEACA,qBAAG,sBAAQA,WAAW,KAAnB,YACH,sBACE,kBAAIA,WAAW,MAAf,qDACA,kBAAIA,WAAW,MAAf,qEAEF,iBAAQ,CACN,GAAM,yBADR,yBAGA,sSACA,4TACA,gKACA,uBAAK,sBAAMA,WAAW,OAAU,IAA3B,ieAiBL,8zBACA,iBAAQ,CACN,GAAM,mCADR,mCAGA,iTACA,sOACA,uBAAK,sBAAMA,WAAW,OAAU,IAA3B,kbAiBL,+JACA,iBAAQ,CACN,GAAM,oBADR,oBAGA,uBAAK,sBAAMA,WAAW,OAAU,IAA3B,iTAaL,iQAAgP,0BAAYA,WAAW,KAAvB,MAAhP,0CAAyU,0BAAYA,WAAW,KAAvB,WAAzU,UAAuY,0BAAYA,WAAW,KAAvB,iBAAvY,KACA,iBAAQ,CACN,GAAM,kBADR,kBAGA,qBAAG,0BAAYA,WAAW,KAAvB,SAAH,wBAA6E,0BAAYA,WAAW,KAAvB,kBAA7E,+BACA,uBAAK,sBAAMA,WAAW,OAAU,IAA3B,sxBA0BL,iBAAQ,CACN,GAAM,uBADR,uBAGA,wEACA,sBACE,kBAAIA,WAAW,MAAf,+BACA,kBAAIA,WAAW,MAAf,sBAA2C,0BAAYA,WAAW,MAAvB,kBAC3C,kBAAIA,WAAW,MAAK,0BAAYA,WAAW,MAAvB,WAApB,qCACA,kBAAIA,WAAW,MAAK,0BAAYA,WAAW,MAAvB,WAApB,cAAuF,0BAAYA,WAAW,MAAvB,cAAvF,OAAsJ,0BAAYA,WAAW,MAAvB,eAAtJ,YAA2N,0BAAYA,WAAW,MAAvB,kBAE7N,oDACA,2CACA,sBACE,kBAAIA,WAAW,MAAf,+DAEF,iEACA,4LACA,8EACA,2CACA,4FACA,0IACA,4CAA2B,0BAAYA,WAAW,KAAvB,4BAA3B,yBACkB,0BAAYA,WAAW,KAAvB,0BAClB,iBAAQ,CACN,GAAM,mBADR,mBAGA,sBACE,kBAAIA,WAAW,MAAf,eAAoC,kBAAIA,WAAW,MAC/C,kBAAIA,WAAW,MAAf,iCACA,kBAAIA,WAAW,MAAf,qBAGN,sBACE,kBAAIA,WAAW,MAAf,+DAEF,sBACE,kBAAIA,WAAW,MAAf,yDACA,kBAAIA,WAAW,MAAf,2DAEF,iBAAQ,CACN,MAAS,GAET,kBAAIA,WAAW,MAAf,yBAEF,sBACE,kBAAIA,WAAW,MAAf,kCACA,kBAAIA,WAAW,MAAf,OAA4B,0BAAYA,WAAW,MAAvB,SAA5B,cACA,kBAAIA,WAAW,MAAf,gDAAqE,kBAAIA,WAAW,MAChF,kBAAIA,WAAW,MAAf,+CAGN,uBAAK,sBAAMA,WAAW,OAAU,IAA3B,mvBAuBL,uBAAK,sBAAMA,WAAW,OAAU,IAA3B,ugBAqBL,uBAAK,sBAAMA,WAAW,OAAU,IAA3B,oyBA6BL,iBAAQ,CACN,GAAM,kCADR,kCAGA,0aACA,uQ,0NAKJJ,EAAWK,gBAAiB","file":"component---manual-machine-learning-machine-learning-intro-md-8feea9e95d514a5c36bc.js","sourcesContent":["import React from 'react'\n  /* @jsx mdx */\nimport { mdx } from '@mdx-js/react';\n/* @jsx mdx */\n\nimport DefaultLayout from \"/Users/dennis.okeeffe/Project-Imposter/developer-notes/node_modules/gatsby-theme-docz/src/base/Layout.js\";\nexport const _frontmatter = {};\n\nconst makeShortcode = name => function MDXDefaultShortcode(props) {\n  console.warn(\"Component \" + name + \" was not imported, exported, or provided by MDXProvider as global scope\");\n  return <div {...props} />;\n};\n\nconst layoutProps = {\n  _frontmatter\n};\nconst MDXLayout = DefaultLayout;\nexport default function MDXContent({\n  components,\n  ...props\n}) {\n  return <MDXLayout {...layoutProps} {...props} components={components} mdxType=\"MDXLayout\">\n\n\n    <h1 {...{\n      \"id\": \"intro-to-machine-learning\"\n    }}>{`Intro to Machine Learning`}</h1>\n    <h2 {...{\n      \"id\": \"table-of-contents\"\n    }}>{`Table of Contents`}</h2>\n    {\n      /* TOC */\n    }\n    <ul>\n      <li parentName=\"ul\"><a parentName=\"li\" {...{\n          \"href\": \"#intro-to-machine-learning\"\n        }}>{`Intro to Machine Learning`}</a><ul parentName=\"li\">\n          <li parentName=\"ul\"><a parentName=\"li\" {...{\n              \"href\": \"#table-of-contents\"\n            }}>{`Table of Contents`}</a></li>\n          <li parentName=\"ul\"><a parentName=\"li\" {...{\n              \"href\": \"#what-is-machine-learning\"\n            }}>{`What is Machine Learning?`}</a><ul parentName=\"li\">\n              <li parentName=\"ul\"><a parentName=\"li\" {...{\n                  \"href\": \"#-----basic-model-prediction\"\n                }}>{`---- Basic Model Prediction`}</a></li>\n            </ul></li>\n          <li parentName=\"ul\"><a parentName=\"li\" {...{\n              \"href\": \"#classification-regression-clustering\"\n            }}>{`Classification, Regression, Clustering`}</a><ul parentName=\"li\">\n              <li parentName=\"ul\"><a parentName=\"li\" {...{\n                  \"href\": \"#-----classification-example-filtering-spam\"\n                }}>{`---- Classification Example: Filtering Spam`}</a></li>\n              <li parentName=\"ul\"><a parentName=\"li\" {...{\n                  \"href\": \"#-----regression-example-linkedin-views\"\n                }}>{`---- Regression Example: LinkedIn Views`}</a></li>\n              <li parentName=\"ul\"><a parentName=\"li\" {...{\n                  \"href\": \"#-----clustering-example-separating-the-iris-species\"\n                }}>{`---- Clustering Example: Separating the Iris Species`}</a></li>\n              <li parentName=\"ul\"><a parentName=\"li\" {...{\n                  \"href\": \"#-----supervised-vs-unsupervised\"\n                }}>{`---- Supervised vs. Unsupervised`}</a></li>\n              <li parentName=\"ul\"><a parentName=\"li\" {...{\n                  \"href\": \"#-----getting-practical-with-supervised-learning\"\n                }}>{`---- Getting practical with supervised learning`}</a></li>\n              <li parentName=\"ul\"><a parentName=\"li\" {...{\n                  \"href\": \"#-----getting-practical-with-unsupervised-learning\"\n                }}>{`---- Getting practical with unsupervised learning`}</a></li>\n            </ul></li>\n          <li parentName=\"ul\"><a parentName=\"li\" {...{\n              \"href\": \"#performance-measures\"\n            }}>{`Performance Measures`}</a><ul parentName=\"li\">\n              <li parentName=\"ul\"><a parentName=\"li\" {...{\n                  \"href\": \"#-----confusion-matrix\"\n                }}>{`---- Confusion Matrix`}</a></li>\n              <li parentName=\"ul\"><a parentName=\"li\" {...{\n                  \"href\": \"#-----calculating-the-rmse-of-air-data\"\n                }}>{`---- Calculating the RMSE of air data`}</a></li>\n              <li parentName=\"ul\"><a parentName=\"li\" {...{\n                  \"href\": \"#-----clustering-dataset-example\"\n                }}>{`---- Clustering dataset example`}</a></li>\n              <li parentName=\"ul\"><a parentName=\"li\" {...{\n                  \"href\": \"#-----training-set-and-test-set\"\n                }}>{`---- Training Set and Test Set`}</a></li>\n              <li parentName=\"ul\"><a parentName=\"li\" {...{\n                  \"href\": \"#-----split-the-sets\"\n                }}>{`---- Split the Sets`}</a></li>\n              <li parentName=\"ul\"><a parentName=\"li\" {...{\n                  \"href\": \"#-----using-cross-validation\"\n                }}>{`---- Using Cross Validation`}</a></li>\n              <li parentName=\"ul\"><a parentName=\"li\" {...{\n                  \"href\": \"#-----bias-and-variance\"\n                }}>{`---- Bias and Variance`}</a></li>\n              <li parentName=\"ul\"><a parentName=\"li\" {...{\n                  \"href\": \"#-----overfitting-the-spam\"\n                }}>{`---- Overfitting the Spam`}</a></li>\n            </ul></li>\n          <li parentName=\"ul\"><a parentName=\"li\" {...{\n              \"href\": \"#classification\"\n            }}>{`Classification`}</a><ul parentName=\"li\">\n              <li parentName=\"ul\"><a parentName=\"li\" {...{\n                  \"href\": \"#-----learn-a-decision-tree\"\n                }}>{`---- Learn a Decision Tree`}</a></li>\n              <li parentName=\"ul\"><a parentName=\"li\" {...{\n                  \"href\": \"#-----classify-with-the-decision-tree\"\n                }}>{`---- Classify with the Decision Tree`}</a></li>\n              <li parentName=\"ul\"><a parentName=\"li\" {...{\n                  \"href\": \"#-----pruning-the-tree\"\n                }}>{`---- Pruning the Tree`}</a></li>\n              <li parentName=\"ul\"><a parentName=\"li\" {...{\n                  \"href\": \"#-----gini-criterion\"\n                }}>{`---- Gini Criterion`}</a></li>\n              <li parentName=\"ul\"><a parentName=\"li\" {...{\n                  \"href\": \"#-----k-nearest-neighbors\"\n                }}>{`---- k-Nearest Neighbors`}</a></li>\n              <li parentName=\"ul\"><a parentName=\"li\" {...{\n                  \"href\": \"#-----scaling-example\"\n                }}>{`---- Scaling Example`}</a></li>\n              <li parentName=\"ul\"><a parentName=\"li\" {...{\n                  \"href\": \"#-----interpreting-a-voronoi-diagram\"\n                }}>{`---- Interpreting a Voronoi Diagram`}</a></li>\n            </ul></li>\n        </ul></li>\n    </ul>\n    {\n      /* /TOC */\n    }\n    <hr></hr>\n    <h2 {...{\n      \"id\": \"what-is-machine-learning\"\n    }}>{`What is Machine Learning?`}</h2>\n    <p>{`Construction/use of algorithms that learn from data.`}</p>\n    <p>{`We decide that it can learn when it has higher performance after learning more information.`}</p>\n    <p>{`Example: label squares based on size and edge.`}</p>\n    <p>{`If some squares were, however, solved by people - then these instances can be used to give an informed reply.`}</p>\n    <p>{`For input knowledge, we can use pre-labeled squares that may give us an indication of which way to go.`}</p>\n    <p>{`We can make ground on this by constructing a data frame.`}</p>\n    <pre><code parentName=\"pre\" {...{}}>{`These can be used in R to get more information.\n\ndim()\nstr()\nsummary()\n`}</code></pre>\n    <p>{`The goal is to build models for prediction.`}</p>\n    <p>{`We can use things like regression to help predict these things.`}</p>\n    <p><em parentName=\"p\">{`Formulation`}</em></p>\n    <p>{`Input -> `}<em parentName=\"p\">{`Estimated Function`}</em>{` -> Output`}</p>\n    <pre><code parentName=\"pre\" {...{}}>{`# Reveal number of observations and variables in two different ways\n> str(iris)\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n> dim(iris)\n[1] 150   5\n>\n>\n# Show first and last observations in the iris data set\n> head(iris)\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n> tail(iris)\n    Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n145          6.7         3.3          5.7         2.5 virginica\n146          6.7         3.0          5.2         2.3 virginica\n147          6.3         2.5          5.0         1.9 virginica\n148          6.5         3.0          5.2         2.0 virginica\n149          6.2         3.4          5.4         2.3 virginica\n150          5.9         3.0          5.1         1.8 virginica\n>\n>\n# Summarize the iris data set\n> summary(iris)\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width\n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n       Species  \n setosa    :50  \n versicolor:50  \n virginica :50  \n`}</code></pre>\n    <h2 {...{\n      \"id\": \"basic-model-prediction\"\n    }}>{`Basic Model Prediction`}</h2>\n    <p>{`You'll be working with the Wage dataset. It contains the wage and some general information for workers in the mid-Atlantic region of the US.`}</p>\n    <p>{`As we briefly discussed in the video, there could be a relationship between a worker's age and his wage. Older workers tend to have more experience on average than their younger counterparts, hence you could expect an increasing trend in wage as workers age. So we built a linear regression model for you, using lm(): lm_wage. This model predicts the wage of a worker based only on the worker's age.`}</p>\n    <p>{`With this linear model lm_wage, which is built with data that contain information on workers' age and their corresponding wage, you can predict the wage of a worker given the age of that worker. For example, suppose you want to predict the wage of a 60 year old worker. You can use the predict() function for this. This generic function takes a model as the first argument. The second argument should be some unseen observations as a data frame. predict() is then able to predict outcomes for these observations.`}</p>\n    <pre><code parentName=\"pre\" {...{}}>{`> str(Wage)\n'data.frame':   3000 obs. of  12 variables:\n $ year      : int  2006 2004 2003 2003 2005 2008 2009 2008 2006 2004 ...\n $ age       : int  18 24 45 43 50 54 44 30 41 52 ...\n $ sex       : Factor w/ 2 levels \"1. Male\",\"2. Female\": 1 1 1 1 1 1 1 1 1 1 ...\n $ maritl    : Factor w/ 5 levels \"1. Never Married\",..: 1 1 2 2 4 2 2 1 1 2 ...\n $ race      : Factor w/ 4 levels \"1. White\",\"2. Black\",..: 1 1 1 3 1 1 4 3 2 1 ...\n $ education : Factor w/ 5 levels \"1. < HS Grad\",..: 1 4 3 4 2 4 3 3 3 2 ...\n $ region    : Factor w/ 9 levels \"1. New England\",..: 2 2 2 2 2 2 2 2 2 2 ...\n $ jobclass  : Factor w/ 2 levels \"1. Industrial\",..: 1 2 1 2 2 2 1 2 2 2 ...\n $ health    : Factor w/ 2 levels \"1. <=Good\",\"2. >=Very Good\": 1 2 1 2 1 2 2 1 2 2 ...\n $ health_ins: Factor w/ 2 levels \"1. Yes\",\"2. No\": 2 2 1 1 1 1 1 1 1 1 ...\n $ logwage   : num  4.32 4.26 4.88 5.04 4.32 ...\n $ wage      : num  75 70.5 131 154.7 75 ...\n>\n# Build Linear Model: lm_wage (coded already)\n> lm_wage <- lm(wage ~ age, data = Wage)\n>\n# Define data.frame: unseen (coded already)\n> unseen <- data.frame(age = 60)\n>\n# Predict the wage for a 60-year old worker\n> predict(lm_wage, unseen)\n       1\n124.1413\n`}</code></pre>\n    <hr></hr>\n    <h2 {...{\n      \"id\": \"classification-regression-clustering\"\n    }}>{`Classification, Regression, Clustering`}</h2>\n    <p>{`These are the three common types of ML Problems.`}</p>\n    <p><strong parentName=\"p\">{`Classification`}</strong></p>\n    <p>{`Predicting category through historical classifying.`}</p>\n    <p><inlineCode parentName=\"p\">{`Earlier Observations`}</inlineCode>{` -> `}<em parentName=\"p\">{`estimate`}</em>{` -> `}<inlineCode parentName=\"p\">{`CLASSIFIER`}</inlineCode></p>\n    <p><inlineCode parentName=\"p\">{`Unseen Data`}</inlineCode>{` -> `}<em parentName=\"p\">{`CLASSIFIER`}</em>{` -> `}<inlineCode parentName=\"p\">{`Class`}</inlineCode></p>\n    <p>{`Application: Medical Diagnosis, Animal Recognition`}</p>\n    <p>{`Important: Qualitative Output, Predefined Classes`}</p>\n    <p><strong parentName=\"p\">{`Regression`}</strong></p>\n    <p>{`We are trying to estimate a function that will render the correct response.`}</p>\n    <p>{`Eg. knowing height and weight, is there a relationship? Is it linear? Can we predict a height given a weight?`}</p>\n    <p><inlineCode parentName=\"p\">{`PREDICTORS`}</inlineCode>{` -> `}<em parentName=\"p\">{`Regression Function`}</em>{` -> `}<inlineCode parentName=\"p\">{`RESPONSE`}</inlineCode></p>\n    <p>{`Application: Modelling Payments for Credit Scores, YouTube Subscriptions over time, Job dependent on Grades.`}</p>\n    <p>{`Important: Quantitative Output, previous input-output observations`}</p>\n    <p><strong parentName=\"p\">{`Clustering`}</strong></p>\n    <p>{`Grouping objects that are `}<inlineCode parentName=\"p\">{`similar`}</inlineCode>{` in clusters and `}<inlineCode parentName=\"p\">{`dissimilar`}</inlineCode>{` between clusters. It's like classification without saying which class an object need to relate to.`}</p>\n    <p>{`Eg. Grouping similar animal photos`}</p>\n    <p>{`There `}<inlineCode parentName=\"p\">{`no labels, no right or wrong, and plenty of possible clusterings`}</inlineCode></p>\n    <p>{`Another example is k-Means can do things like cluster in similar groups.`}</p>\n    <h2 {...{\n      \"id\": \"classification-example-filtering-spam\"\n    }}>{`Classification Example: Filtering Spam`}</h2>\n    <p>{`In the following exercise you'll work with the dataset emails, which is loaded in your workspace (Source: UCI Machine Learning Repository). Here, several emails have been labeled by humans as spam (1) or not spam (0) and the results are found in the column spam. The considered feature in emails to predict whether it was spam or not is avgCapitalSeq. It is the average amount of sequential capital letters found in each email.`}</p>\n    <p>{`In the code, you'll find a crude spam filter we built for you, spamClassifier() that uses avgCapitalSeq to predict whether an email is spam or not. In the function definition, it's important to realize that x refers to avgCapitalSeq. So where the avgCapitalSeq is greater than 4, spamClassifier() predicts the email is spam (1), if avgCapitalSeq is inclusively between 3 and 4, it predicts not spam (0), and so on. This classifier's methodology of predicting whether an email is spam or not seems pretty random, but let's see how it does anyways!`}</p>\n    <p>{`Your job is to inspect the emails dataset, apply spamClassifier to it, and compare the predicted labels with the true labels.`}</p>\n    <pre><code parentName=\"pre\" {...{}}>{`# Show the dimensions of emails\n> dim(emails)\n[1] 13  2\n>\n# Inspect definition of spam_classifier()\n> spam_classifier <- function(x){\n    prediction <- rep(NA, length(x)) # initialize prediction vector\n    prediction[x > 4] <- 1\n    prediction[x >= 3 & x <= 4] <- 0\n    prediction[x >= 2.2 & x < 3] <- 1\n    prediction[x >= 1.4 & x < 2.2] <- 0\n    prediction[x > 1.25 & x < 1.4] <- 1\n    prediction[x <= 1.25] <- 0\n    return(prediction) # prediction is either 0 or 1\n  }\n>\n# Apply the classifier to the avgCapitalSeq column: spam_pred\n> spamPred <- sapply(emails$avgCapitalSeq, spamClassifier)\n>\n# Compare spam_pred to emails$spam. Use ==\n> spam_pred == emails$spam\n [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n`}</code></pre>\n    <h2 {...{\n      \"id\": \"regression-example-linkedin-views\"\n    }}>{`Regression Example: LinkedIn Views`}</h2>\n    <p>{`It's time for you to make another prediction with regression! More precisely, you'll analyze the number of views of your LinkedIn profile. With your growing network and your data science skills improving daily, you wonder if you can predict how often your profile will be visited in the future based on the number of days it's been since you created your LinkedIn account.`}</p>\n    <p>{`The instructions will help you predict the number of profile views for the next 3 days, based on the views for the past 3 weeks. The linkedin vector, which contains this information, is already available in your workspace.`}</p>\n    <pre><code parentName=\"pre\" {...{}}>{`# linkedin is already available in your workspace\n>\n# Create the days vector\n> days <- c(seq(1:21))\n>\n# Fit a linear model called on the linkedin views per day: linkedin_lm\n> linkedin_lm <- lm(linkedin ~ days)\n>\n# Predict the number of views for the next three days: linkedin_pred\n> future_days <- data.frame(days = 22:24)\n> linkedin_pred <- predict(linkedin_lm, future_days)\n>\n# Plot historical data and predictions\n> plot(linkedin ~ days, xlim = c(1, 24))\n> points(22:24, linkedin_pred, col = \"green\")\n`}</code></pre>\n    <h2 {...{\n      \"id\": \"clustering-example-separating-the-iris-species\"\n    }}>{`Clustering Example: Separating the Iris Species`}</h2>\n    <p>{`Last but not least, there's clustering. This technique tries to group your objects. It does this without any prior knowledge of what these groups could or should look like. For clustering, the concepts of prior knowledge and unseen observations are less meaningful than for classification and regression.`}</p>\n    <p>{`In this exercise, you'll group irises in 3 distinct clusters, based on several flower characteristics in the iris dataset. It has already been chopped up in a data frame my_iris and a vector species, as shown in the sample code on the right.`}</p>\n    <p>{`The clustering itself will be done with the kmeans() function. How the algorithm actually works, will be explained in the last chapter. For now, just try it out to gain some intuition!`}</p>\n    <p>{`Note: In problems that have a random aspect (like this problem with kmeans()), the set.seed() function will be used to enforce reproducibility. If you fix the seed, the random numbers that are generated (e.g. in kmeans()) are always the same.`}</p>\n    <pre><code parentName=\"pre\" {...{}}>{`# Set random seed. Don't remove this line.\n> set.seed(1)\n>\n# Chop up iris in my_iris and species\n> my_iris <- iris[-5]\n> species <- iris$Species\n>\n# Perform k-means clustering on my_iris: kmeans_iris\n> kmeans_iris <- kmeans(my_iris, 3)\n>\n# Compare the actual Species to the clustering using table()\n> table(species, kmeans_iris$cluster)\n\nspecies       1  2  3\n  setosa     50  0  0\n  versicolor  0  2 48\n  virginica   0 36 14\n>\n# Plot Petal.Width against Petal.Length, coloring by cluster\n> plot(Petal.Length ~ Petal.Width, data = my_iris, col = kmeans_iris$cluster)\n`}</code></pre>\n    <h2 {...{\n      \"id\": \"supervised-vs-unsupervised\"\n    }}>{`Supervised vs. Unsupervised`}</h2>\n    <p>{`Classification and Regression have similar traits.`}</p>\n    <p>{`If we can `}<inlineCode parentName=\"p\">{`find`}</inlineCode>{` function f which can be used to assign a class or value to unseen observations `}<inlineCode parentName=\"p\">{`given`}</inlineCode>{` a set of labeled observations, we call this `}<inlineCode parentName=\"p\">{`Supervised Learning`}</inlineCode>{`.`}</p>\n    <p><inlineCode parentName=\"p\">{`Labelling`}</inlineCode>{` can be tedious and are normally done by humans. Those that don't require labels is known as `}<inlineCode parentName=\"p\">{`Unsupervised Learning`}</inlineCode>{` - example being the clustering that we did before. Clustering will find group observations that are similar.`}</p>\n    <p><strong parentName=\"p\">{`Performance of the model`}</strong></p>\n    <ul>\n      <li parentName=\"ul\">{`Supervised learning - `}<inlineCode parentName=\"li\">{`Compare`}</inlineCode>{` real labels with `}<inlineCode parentName=\"li\">{`predicted`}</inlineCode>{` labels`}</li>\n      <li parentName=\"ul\">{`Unsupervised Learning - No real labels to compare - Techniques will be explained later down the track - Things aren't always black and white`}</li>\n      <li parentName=\"ul\">{`Semi-Supervised Learning - Mixed of unlabeled and labeled observationed - Eg clustering information and classes of labeled observations to assign a class to unlabeled observations - More labeled observations for `}<inlineCode parentName=\"li\">{`supervised learning`}</inlineCode></li>\n    </ul>\n    <h2 {...{\n      \"id\": \"getting-practical-with-supervised-learning\"\n    }}>{`Getting practical with supervised learning`}</h2>\n    <p>{`In this exercise, you will use the same dataset. But instead of dropping the Species labels, you will use them do some supervised learning using recursive partitioning! Don't worry if you don't know what that is yet. Recursive partitioning (a.k.a. decision trees) will be explained in Chapter 3.`}</p>\n    <p>{`Take a look at the iris dataset, using str() and summary().`}</p>\n    <p>{`The code that builds a supervised learning model with the rpart() function from the rpart package is already provided for you. This model trains a decision tree on the iris dataset.`}</p>\n    <p>{`Use the predict() function with the tree model as the first argument. The second argument should be a data frame containing observations of which you want to predict the label. In this case, you can use the predefined unseen data frame. The third argument should be type = \"class\".`}</p>\n    <p>{`Simply print out the result of this prediction step.`}</p>\n    <pre><code parentName=\"pre\" {...{}}>{`# Set random seed. Don't remove this line.\n> set.seed(1)\n>\n# Take a look at the iris dataset\n> str(iris)\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n> summary(iris)\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width\n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n       Species  \n setosa    :50  \n versicolor:50  \n virginica :50\n>\n# A decision tree model has been built for you\n> tree <- rpart(Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width,\n                data = iris, method = \"class\")\n>\n# A dataframe containing unseen observations\n> unseen <- data.frame(Sepal.Length = c(5.3, 7.2),\n                       Sepal.Width = c(2.9, 3.9),\n                       Petal.Length = c(1.7, 5.4),\n                       Petal.Width = c(0.8, 2.3))\n>\n# Predict the label of the unseen observations. Print out the result.\n> predict(tree, unseen, type=\"class\")\n        1         2\n   setosa virginica\nLevels: setosa versicolor virginica\n`}</code></pre>\n    <h2 {...{\n      \"id\": \"getting-practical-with-unsupervised-learning\"\n    }}>{`Getting practical with unsupervised learning`}</h2>\n    <pre><code parentName=\"pre\" {...{}}>{`> head(cars)\n                     wt  hp\nMazda RX4         2.620 110\nMazda RX4 Wag     2.875 110\nDatsun 710        2.320  93\nHornet 4 Drive    3.215 110\nHornet Sportabout 3.440 175\nValiant           3.460 105\n> # The cars data frame is pre-loaded\n>\n> # Set random seed. Don't remove this line.\n> set.seed(1)\n>\n> # Explore the cars dataset\n>\n> str(cars)\n'data.frame':   32 obs. of  2 variables:\n $ wt: num  2.62 2.88 2.32 3.21 3.44 ...\n $ hp: num  110 110 93 110 175 105 245 62 95 123 ...\n> summary(cars)\n       wt              hp\n Min.   :1.513   Min.   : 52.0  \n 1st Qu.:2.581   1st Qu.: 96.5  \n Median :3.325   Median :123.0  \n Mean   :3.217   Mean   :146.7  \n 3rd Qu.:3.610   3rd Qu.:180.0  \n Max.   :5.424   Max.   :335.0  \n>\n> # Group the dataset into two clusters: km_cars\n> km_cars <- kmeans(cars, 2)\n>\n> # Print out the contents of each cluster\n> km_cars$cluster\n          Mazda RX4       Mazda RX4 Wag          Datsun 710      Hornet 4 Drive\n                  1                   1                   1                   1\n  Hornet Sportabout             Valiant          Duster 360           Merc 240D\n                  2                   1                   2                   1\n           Merc 230            Merc 280           Merc 280C          Merc 450SE\n                  1                   1                   1                   2\n         Merc 450SL         Merc 450SLC  Cadillac Fleetwood Lincoln Continental\n                  2                   2                   2                   2\n  Chrysler Imperial            Fiat 128         Honda Civic      Toyota Corolla\n                  2                   1                   1                   1\n      Toyota Corona    Dodge Challenger         AMC Javelin          Camaro Z28\n                  1                   1                   1                   2\n   Pontiac Firebird           Fiat X1-9       Porsche 914-2        Lotus Europa\n                  2                   1                   1                   1\n     Ford Pantera L        Ferrari Dino       Maserati Bora          Volvo 142E\n                  2                   2                   2                   1\n\n# see km_cars in general\n> km_cars\nK-means clustering with 2 clusters of sizes 19, 13\n\nCluster means:\n        wt        hp\n1 2.692000  99.47368\n2 3.984923 215.69231\n\nClustering vector:\n          Mazda RX4       Mazda RX4 Wag          Datsun 710      Hornet 4 Drive\n                  1                   1                   1                   1\n  Hornet Sportabout             Valiant          Duster 360           Merc 240D\n                  2                   1                   2                   1\n           Merc 230            Merc 280           Merc 280C          Merc 450SE\n                  1                   1                   1                   2\n         Merc 450SL         Merc 450SLC  Cadillac Fleetwood Lincoln Continental\n                  2                   2                   2                   2\n  Chrysler Imperial            Fiat 128         Honda Civic      Toyota Corolla\n                  2                   1                   1                   1\n      Toyota Corona    Dodge Challenger         AMC Javelin          Camaro Z28\n                  1                   1                   1                   2\n   Pontiac Firebird           Fiat X1-9       Porsche 914-2        Lotus Europa\n                  2                   1                   1                   1\n     Ford Pantera L        Ferrari Dino       Maserati Bora          Volvo 142E\n                  2                   2                   2                   1\n\nWithin cluster sum of squares by cluster:\n[1] 14085.06 27403.23\n (between_SS / total_SS =  71.5 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"\n`}</code></pre>\n    <p>{`An important part in machine learning is understanding your results. In the case of clustering, visualization is key to interpretation! One way to achieve this is by plotting the features of the cars and coloring the points based on their corresponding cluster.`}</p>\n    <p>{`In this exercise you'll summarize your results in a comprehensive figure. The dataset cars is already available in your workspace; the code to perform the clustering is already available.`}</p>\n    <pre><code parentName=\"pre\" {...{}}>{`# The cars data frame is pre-loaded\n>\n# Set random seed. Don't remove this line\n> set.seed(1)\n>\n# Group the dataset into two clusters: km_cars\n> km_cars <- kmeans(cars, 2)\n>\n# Add code: color the points in the plot based on the clusters\n> plot(cars, col=km_cars$cluster)\n>\n# Print out the cluster centroids\n> km_cars$centers\n        wt        hp\n1 2.692000  99.47368\n2 3.984923 215.69231\n>\n# Replace the ___ part: add the centroids to the plot\n> points(km_cars$centers, pch = 22, bg = c(1, 2), cex = 2)\n`}</code></pre>\n    <hr></hr>\n    <h2 {...{\n      \"id\": \"performance-measures\"\n    }}>{`Performance Measures`}</h2>\n    <p>{`How is our model any good? It depends on how you define performance. This could be...`}</p>\n    <ul>\n      <li parentName=\"ul\">{`Accuracy`}</li>\n      <li parentName=\"ul\">{`Computation Time`}</li>\n      <li parentName=\"ul\">{`Interpretability`}</li>\n    </ul>\n    <p><strong parentName=\"p\">{`Classification Testing`}</strong></p>\n    <p>{`Accuray and Error are how we can help define classification performance.`}</p>\n    <p><inlineCode parentName=\"p\">{`Accuray = corrct / total`}</inlineCode></p>\n    <p>{`Eg. Square with 2 features. If each square can be coloured/not coloured (binary classification problem)`}</p>\n    <p>{`If the model only classifies 3/5 correct, then that is our accuracy (60%).`}</p>\n    <p><em parentName=\"p\">{`Limits of accuracy`}</em></p>\n    <p>{`Confusion matrix: rows and columns with each available labels.`}</p>\n    <p>{`Each cell contains frequency of instances that are classified in a certain way.`}</p>\n    <p>{`For a binary classifier, we have positive or negative in this case (1 or 0). Our matrix then becomes a square table of Truth vs. Prediction. TP, FN, FP, TN.`}</p>\n    <p>{`From this we can calculation `}<inlineCode parentName=\"p\">{`Precision as TP/(TP+FP)`}</inlineCode>{` and `}<inlineCode parentName=\"p\">{`Recall is TP/(TP+FN)`}</inlineCode>{`. Back on the square example, we can talk about which were correctly classified.`}</p>\n    <p>{`Accuracy calculation then becomes `}<inlineCode parentName=\"p\">{`(TP+TN)/sum(all squares)`}</inlineCode>{`.`}</p>\n    <p>{`This means for the `}<inlineCode parentName=\"p\">{`rare heart disease`}</inlineCode>{` example, we could be looking at a recall of 0% and other results that are `}<inlineCode parentName=\"p\">{`undefined`}</inlineCode>{`.`}</p>\n    <p><strong parentName=\"p\">{`Regression Testing`}</strong></p>\n    <p>{`RMSE: Root Mean Squared Error.`}</p>\n    <p><strong parentName=\"p\">{`Clustering Testing`}</strong></p>\n    <p>{`Here, we have no label info, so we need to go with distance metrics between points.`}</p>\n    <p>{`Performance measure consists of 2 elements.`}</p>\n    <ol>\n      <li parentName=\"ol\">{`Similarity within each cluster - we want this to be high`}</li>\n      <li parentName=\"ol\">{`Similarity between clusters - we want this to be low`}</li>\n    </ol>\n    <p>{`There are a number techniques.`}</p>\n    <p>{`Within clusters:`}</p>\n    <ul>\n      <li parentName=\"ul\">{`Within sum of squares(WSS)`}</li>\n      <li parentName=\"ul\">{`Diameter`}</li>\n      <li parentName=\"ul\">{`Minimize`}</li>\n    </ul>\n    <p>{`Between clusters:`}</p>\n    <ul>\n      <li parentName=\"ul\">{`Between cluster sum of squares (BSS)`}</li>\n      <li parentName=\"ul\">{`Intercluster distance`}</li>\n      <li parentName=\"ul\">{`Maximise`}</li>\n    </ul>\n    <p>{`A popular index for comparing is the Dunn's index: `}<inlineCode parentName=\"p\">{`minimal intercluster distance/maximal diameter`}</inlineCode></p>\n    <h2 {...{\n      \"id\": \"confusion-matrix\"\n    }}>{`Confusion Matrix`}</h2>\n    <p>{`In this exercise, a decision tree is learned on this dataset. The tree aims to predict whether a person would have survived the accident based on the variables Age, Sex and Pclass (travel class). The decision the tree makes can be deemed correct or incorrect if we know what the person's true outcome was. That is, if it's a supervised learning problem.`}</p>\n    <p>{`Since the true fate of the passengers, Survived, is also provided in titanic, you can compare it to the prediction made by the tree. As you've seen in the video, the results can be summarized in a confusion matrix. In R, you can use the table() function for this.`}</p>\n    <p>{`In this exercise, you will only focus on assessing the performance of the decision tree. In chapter 3, you will learn how to actually build a decision tree yourself.`}</p>\n    <p>{`Note: As in the previous chapter, there are functions that have a random aspect. The set.seed() function is used to enforce reproducibility. Don't worry about it, just don't remove it!`}</p>\n    <pre><code parentName=\"pre\" {...{}}>{`# The titanic dataset is already loaded into your workspace\n>\n# Set random seed. Don't remove this line\n> set.seed(1)\n>\n# Have a look at the structure of titanic\n> str(titanic)\n'data.frame':   714 obs. of  4 variables:\n $ Survived: Factor w/ 2 levels \"1\",\"0\": 2 1 1 1 2 2 2 1 1 1 ...\n $ Pclass  : int  3 1 3 1 3 1 3 3 2 3 ...\n $ Sex     : Factor w/ 2 levels \"female\",\"male\": 2 1 1 1 2 2 2 1 1 1 ...\n $ Age     : num  22 38 26 35 35 54 2 27 14 4 ...\n>\n# A decision tree classification model is built on the data\n> tree <- rpart(Survived ~ ., data = titanic, method = \"class\")\n>\n# Use the predict() method to make predictions, assign to pred\n> pred <- predict(tree, titanic, type=\"class\")\n>\n# Use the table() method to make the confusion matrix\n> table(titanic$Survived, pred)\n   pred\n      1   0\n  1 212  78\n  0  53 371\n`}</code></pre>\n    <p>{`The confusion matrix from the last exercise provides you with the raw performance of the decision tree:`}</p>\n    <p>{`The survivors correctly predicted to have survived: true positives (TP)\nThe deceased who were wrongly predicted to have survived: false positives (FP)\nThe survivors who were wrongly predicted to have perished: false negatives (FN)\nThe deceased who were correctly predicted to have perished: true negatives (TN)`}</p>\n    <pre><code parentName=\"pre\" {...{}}>{`> conf\n\n      1   0\n  1 212  78\n  0  53 371\n# The confusion matrix is available in your workspace as conf\n>\n# Assign TP, FN, FP and TN using conf\n> TP <- conf[1, 1] # this will be 212\n> FN <- conf[1, 2] # this will be 78\n> FP <- conf[2, 1] # fill in\n> TN <- conf[2, 2] # fill in\n>\n# Calculate and print the accuracy: acc\n> acc <- (TP + TN) / (TP + FN + FP + TN)\n> acc\n[1] 0.8165266\n>\n# Calculate and print out the precision: prec\n> prec <- TP/(TP+FP)\n> prec\n[1] 0.8\n>\n# Calculate and print out the recall: rec\n> rec <- TP/(TP+FN)\n> rec\n[1] 0.7310345\n`}</code></pre>\n    <h2 {...{\n      \"id\": \"calculating-the-rmse-of-air-data\"\n    }}>{`Calculating the RMSE of air data`}</h2>\n    <pre><code parentName=\"pre\" {...{}}>{`# The air dataset is already loaded into your workspace\n>\n# Take a look at the structure of air\n> str(air)\n'data.frame':   1503 obs. of  6 variables:\n $ freq     : int  800 1000 1250 1600 2000 2500 3150 4000 5000 6300 ...\n $ angle    : num  0 0 0 0 0 0 0 0 0 0 ...\n $ ch_length: num  0.305 0.305 0.305 0.305 0.305 ...\n $ velocity : num  71.3 71.3 71.3 71.3 71.3 71.3 71.3 71.3 71.3 71.3 ...\n $ thickness: num  0.00266 0.00266 0.00266 0.00266 0.00266 ...\n $ dec      : num  126 125 126 128 127 ...\n>\n# Inspect your colleague's code to build the model\n> fit <- lm(dec ~ freq + angle + ch_length, data = air)\n>\n# Use the model to predict for all values: pred\n> pred <- predict(fit)\n>\n# Use air$dec and pred to calculate the RMSE\n> rmse <- sqrt((1/nrow(air)) * sum( (air$dec - pred) ^ 2))\n>\n# Print out rmse\n> rmse\n[1] 5.215778\n`}</code></pre>\n    <p>{`Using the `}<inlineCode parentName=\"p\">{`rmse`}</inlineCode>{` result for comparison with another result`}</p>\n    <pre><code parentName=\"pre\" {...{}}>{`# Previous model\n> fit <- lm(dec ~ freq + angle + ch_length, data = air)\n> pred <- predict(fit)\n> rmse <- sqrt(sum( (air$dec - pred) ^ 2) / nrow(air))\n> rmse\n[1] 5.215778\n>\n# Your colleague's more complex model\n> fit2 <- lm(dec ~ freq + angle + ch_length + velocity + thickness, data = air)\n>\n# Use the model to predict for all values: pred2\n> pred2 <- predict(fit2)\n>\n# Calculate rmse2\n> rmse2 <- sqrt(sum( (air$dec - pred2) ^ 2) / nrow(air))\n>\n# Print out rmse2\n> rmse2\n[1] 4.799244\n`}</code></pre>\n    <p>{`Adding complexity seems to have caused the RMSE to decrease, from 5.216 to 4.799. But there's more going on here; perhaps adding more variables to a regression always leads to a decrease of your RMSE? There will be more on this later.`}</p>\n    <h2 {...{\n      \"id\": \"clustering-dataset-example\"\n    }}>{`Clustering dataset example`}</h2>\n    <p>{`In the dataset seeds you can find various metrics such as area, perimeter and compactness for 210 seeds. (Source: UCIMLR). However, the seeds' labels were lost. Hence, we don't know which metrics belong to which type of seed. What we do know, is that there were three types of seeds.`}</p>\n    <p>{`The code on the right groups the seeds into three clusters (km_seeds), but is it likely that these three clusters represent our seed types? Let's find out.`}</p>\n    <p>{`There are two initial steps you could take:`}</p>\n    <ol>\n      <li parentName=\"ol\">\n        <p parentName=\"li\">{`Visualize the distribution of cluster assignments among two variables, for example length and compactness.`}</p>\n      </li>\n      <li parentName=\"ol\">\n        <p parentName=\"li\">{`Verify if the clusters are well separated and compact. To do this, you can calculate the between and within cluster sum of squares respectively.`}</p>\n      </li>\n    </ol>\n    <pre><code parentName=\"pre\" {...{}}>{`# The seeds dataset is already loaded into your workspace\n>\n# Set random seed. Don't remove this line\n> set.seed(1)\n>\n# Explore the structure of the dataset\n> str(seeds)\n'data.frame':   210 obs. of  7 variables:\n $ area         : num  15.3 14.9 14.3 13.8 16.1 ...\n $ perimeter    : num  14.8 14.6 14.1 13.9 15 ...\n $ compactness  : num  0.871 0.881 0.905 0.895 0.903 ...\n $ length       : num  5.76 5.55 5.29 5.32 5.66 ...\n $ width        : num  3.31 3.33 3.34 3.38 3.56 ...\n $ asymmetry    : num  2.22 1.02 2.7 2.26 1.35 ...\n $ groove_length: num  5.22 4.96 4.83 4.8 5.17 ...\n>\n# Group the seeds in three clusters\n> km_seeds <- kmeans(seeds, 3)\n>\n# Color the points in the plot based on the clusters\n> plot(length ~ compactness, data = seeds, col = km_seeds$cluster)\n>\n# Print out the ratio of the WSS to the BSS\n> km_seeds$tot.withinss / km_seeds$betweenss\n[1] 0.2762846\n`}</code></pre>\n    <p>{`The within sum of squares is far lower than the between sum of squares. Indicating the clusters are well seperated and overall compact. This is further strengthened by the plot you made, where the clusters you made were visually distinct for these two variables. It's likely that these three clusters represent the three seed types well, even if there's no way to truly verify this.`}</p>\n    <h2 {...{\n      \"id\": \"training-set-and-test-set\"\n    }}>{`Training Set and Test Set`}</h2>\n    <p>{`Looking at the different between supervised learning, Machine learning and other data models.`}</p>\n    <p>{`Supervised learning will have a strong predictive power. - unseen observations`}</p>\n    <p>{`Classical statistics: model must fit data - explain or describe data`}</p>\n    <p>{`Predictive Model - Training - `}<inlineCode parentName=\"p\">{`not`}</inlineCode>{` on complete dataset - training set - `}<inlineCode parentName=\"p\">{`Test set`}</inlineCode>{` to evaluate performance of model - Sets are `}<inlineCode parentName=\"p\">{`disjoint`}</inlineCode>{` - NO OVERLAP - Model testing on `}<inlineCode parentName=\"p\">{`unseen`}</inlineCode>{` observations - Generalization!`}</p>\n    <p><strong parentName=\"p\">{`Split the dataset`}</strong></p>\n    <p>{`Assume you have a dataset with N observations: x, K features: F and Class labels: y.`}</p>\n    <p>{`We can break this down into a training set and a test set.`}</p>\n    <p>{`The test set are used for the observations from x(r+1).`}</p>\n    <p><strong parentName=\"p\">{`When do we use this?`}</strong></p>\n    <p>{`Only important for supervised learning set. It would not be relevant to things like clustering where the data itself isn't labelled.`}</p>\n    <p><strong parentName=\"p\">{`How to split the sets?`}</strong></p>\n    <p>{`The `}<inlineCode parentName=\"p\">{`training set`}</inlineCode>{` should be larger than the `}<inlineCode parentName=\"p\">{`test set`}</inlineCode>{`. Typically a ratio of 3:1 - although this is arbitrary. The more data you use to train, the better the model. Although, we still don't want the `}<inlineCode parentName=\"p\">{`test set`}</inlineCode>{` to be too small!`}</p>\n    <p>{`Wisely choose which elements you put into these sets. They should have similar distributions. Avoid a class not being in a set.`}</p>\n    <p><em parentName=\"p\">{`Regression and Classification`}</em>{` - it is always a smart idea to shuffle the data set before splitting it.`}</p>\n    <p><em parentName=\"p\">{`Effect of smapling`}</em>{` - sampling can affect performance measures. Add `}<inlineCode parentName=\"p\">{`robustness`}</inlineCode>{` to these measures with `}<inlineCode parentName=\"p\">{`cross-validation`}</inlineCode>{`.`}</p>\n    <p><em parentName=\"p\">{`Cross-validation`}</em>{` - Eg. 4-folds validation. This means the splitting the data set and doing this for 4-folds. - n-fold validation means doing this n times with each test set being 1/n large.`}</p>\n    <h2 {...{\n      \"id\": \"split-the-sets\"\n    }}>{`Split the Sets`}</h2>\n    <p>{`In exercises 2 and 3 you calculated a confusion matrix to assess the tree's performance. However, the tree was built using the entire set of observations. Therefore, the confusion matrix doesn't assess the predictive power of the tree. The training set and the test set were one and the same thing: this can be improved!`}</p>\n    <p>{`First, you'll want to split the dataset into train and test sets. You'll notice that the titanic dataset is sorted on titanic$Survived , so you'll need to first shuffle the dataset in order to have a fair distribution of the output variable in each set.`}</p>\n    <p>{`For example, you could use the following commands to shuffle a data frame df and divide it into training and test sets with a 60/40 split between the two.`}</p>\n    <pre><code parentName=\"pre\" {...{}}>{`n <- nrow(df)\nshuffled_df <- df[sample(n), ]\ntrain_indices <- 1:round(0.6 * n)\ntrain <- shuffled_df[train_indices, ]\ntest_indices <- (round(0.6 * n) + 1):n\ntest <- shuffled_df[test_indices, ]\n`}</code></pre>\n    <pre><code parentName=\"pre\" {...{}}>{`# The titanic dataset is already loaded into your workspace\n>\n# Set random seed. Don't remove this line.\n> set.seed(1)\n>\n# Shuffle the dataset, call the result shuffled\n> n <- nrow(titanic)\n> shuffled <- titanic[sample(n),]\n>\n# Split the data in train and test\n> train_indices <- 1:round(0.7 * n)\n> train <- shuffled[train_indices, ]\n> test_indices <- (round(0.7 * n) + 1):n\n> test <- shuffled[test_indices, ]\n>\n# Print the structure of train and test\n> str(train)\n'data.frame':   500 obs. of  4 variables:\n $ Survived: Factor w/ 2 levels \"1\",\"0\": 2 2 2 1 2 1 1 1 1 2 ...\n $ Pclass  : int  3 3 2 1 3 1 2 3 2 3 ...\n $ Sex     : Factor w/ 2 levels \"female\",\"male\": 2 2 1 2 2 2 1 2 1 2 ...\n $ Age     : num  32 19 44 27 7 56 48 9 29 26 ...\n> str(test)\n'data.frame':   214 obs. of  4 variables:\n $ Survived: Factor w/ 2 levels \"1\",\"0\": 1 2 2 1 2 2 2 2 2 2 ...\n $ Pclass  : int  2 3 2 2 1 1 3 3 2 3 ...\n $ Sex     : Factor w/ 2 levels \"female\",\"male\": 1 2 2 1 2 2 2 2 2 1 ...\n $ Age     : num  18 16 36 45 61 31 40.5 28 30 2 ...\n`}</code></pre>\n    <p>{`Time to redo the model training from before. The titanic data frame is again available in your workspace. This time, however, you'll want to build a decision tree on the training set, and next assess its predictive power on a set that has not been used for training: the test set.`}</p>\n    <p>{`On the right, the code that splits titanic up in train and test has already been included. Also, the old code that builds a decision tree on the entire set is included. Up to you to correct it and connect the dots to get a good estimate of the model's predictive ability.`}</p>\n    <pre><code parentName=\"pre\" {...{}}>{`# The titanic dataset is already loaded into your workspace\n>\n# Set random seed. Don't remove this line.\n> set.seed(1)\n>\n# Shuffle the dataset; build train and test\n> n <- nrow(titanic)\n> shuffled <- titanic[sample(n),]\n> train <- shuffled[1:round(0.7 * n),]\n> test <- shuffled[(round(0.7 * n) + 1):n,]\n>\n# Fill in the model that has been learned.\n> tree <- rpart(Survived ~ ., train, method = \"class\")\n>\n# Predict the outcome on the test set with tree: pred\n> pred <- predict(tree, test, type=\"class\")\n>\n# Calculate the confusion matrix: conf\n> conf <- table(test$Survived, pred)\n>\n# Print this confusion matrix\n> conf\n   pred\n      1   0\n  1  58  31\n  0  23 102\n`}</code></pre>\n    <h2 {...{\n      \"id\": \"using-cross-validation\"\n    }}>{`Using Cross Validation`}</h2>\n    <p>{`In this exercise, you will fold the dataset 6 times and calculate the accuracy for each fold. The mean of these accuracies forms a more robust estimation of the model's true accuracy of predicting unseen data, because it is less dependent on the choice of training and test sets.`}</p>\n    <p>{`Note: Other performance measures, such as recall or precision, could also be used here.`}</p>\n    <pre><code parentName=\"pre\" {...{}}>{`# Set random seed. Don't remove this line.\n> set.seed(1)\n>\n# Initialize the accs vector\n> accs <- rep(0,6)\n>\n> for (i in 1:6) {\n    # These indices indicate the interval of the test set\n    indices <- (((i-1) * round((1/6)*nrow(shuffled))) + 1):((i*round((1/6) * nrow(shuffled))))\n\n    # Exclude them from the train set\n    train <- shuffled[-indices,]\n\n    # Include them in the test set\n    test <- shuffled[indices,]\n\n    # A model is learned using each training set\n    tree <- rpart(Survived ~ ., train, method = \"class\")\n\n    # Make a prediction on the test set using tree\n    pred <- predict(tree, test, type=\"class\")\n\n    # Assign the confusion matrix to conf\n    conf <- table(test$Survived, pred)\n\n    # Assign the accuracy of this model to the ith index in accs\n    accs[i] <- sum(diag(conf))/sum(conf)\n  }\n>\n> accs\n[1] 0.7983193 0.7983193 0.7899160 0.8067227 0.8235294 0.7899160\n# Print out the mean of accs\n> mean(accs)\n[1] 0.8011204\n`}</code></pre>\n    <p>{`This estimate will be a more robust measure of your accuracy. It will be less susceptible to the randomness of splitting the dataset.`}</p>\n    <h2 {...{\n      \"id\": \"bias-and-variance\"\n    }}>{`Bias and Variance`}</h2>\n    <p>{`How does splitting affect the accuracy?`}</p>\n    <p>{`We use `}<inlineCode parentName=\"p\">{`Bias`}</inlineCode>{` and `}<inlineCode parentName=\"p\">{`Variance`}</inlineCode>{` as our keys.`}</p>\n    <p>{`The main goal of course is `}<inlineCode parentName=\"p\">{`prediction`}</inlineCode>{`. The `}<inlineCode parentName=\"p\">{`prediction error`}</inlineCode>{` can be split into the `}<inlineCode parentName=\"p\">{`reducible error`}</inlineCode>{` and the `}<inlineCode parentName=\"p\">{`irreducible error`}</inlineCode>{`.`}</p>\n    <p>{`Irreducible: noise - don't minimize!\nReducible: error due to unfit model - this we want to minimize!`}</p>\n    <p><em parentName=\"p\">{`Bias Error`}</em></p>\n    <p>{`Error due to bias: wrong assumptions.\nDifference in predictions and truth. - using models trained by specific `}<inlineCode parentName=\"p\">{`learning algorithm`}</inlineCode></p>\n    <p>{`Eg. suppose you have points on a x/y map that can be fit by quadratic data. If you decide to use linear regression here, you will have a high error since you are restricting your model.`}</p>\n    <p><em parentName=\"p\">{`Variance Error`}</em></p>\n    <p>{`Error due to variance: error due to the sampling of the `}<inlineCode parentName=\"p\">{`training set`}</inlineCode>{`\nModel with high variance fits training set closely!`}</p>\n    <p>{`Example: quadratic data.`}</p>\n    <p>{`It may fit the model well - there will be few restrictions but high variance. If you change the training set, the model will change completely.`}</p>\n    <p><em parentName=\"p\">{`Bias/Variance Tradeoff`}</em></p>\n    <pre><code parentName=\"pre\" {...{}}>{`Low bias = high variance\nLow variance = high bias\n`}</code></pre>\n    <p><strong parentName=\"p\">{`Overfitting and Underfitting`}</strong></p>\n    <p><inlineCode parentName=\"p\">{`Accuracy`}</inlineCode>{` will depend on dataset split (train/test)\nHigh variance will heavily depend on split.`}</p>\n    <p>{`Overfitting = model fits training set a lot better than test set`}</p>\n    <p><inlineCode parentName=\"p\">{`The model is too specific`}</inlineCode></p>\n    <p>{`Underfitting = restricting the model too much`}</p>\n    <p>{`Eg. if you need to decide if email is spam.`}</p>\n    <p>{`Email Training set - exception with 50 capital letters and 30 exclamation marks.\n-> capital letters\n-> exclamation marks`}</p>\n    <p>{`Our trust set has yes to both of the above data sets are spam and not if no.`}</p>\n    <p>{`An `}<inlineCode parentName=\"p\">{`underfit`}</inlineCode>{` model may mark spam if more than 10 capital letters. This is `}<inlineCode parentName=\"p\">{`too general`}</inlineCode>{`.`}</p>\n    <h2 {...{\n      \"id\": \"overfitting-the-spam\"\n    }}>{`Overfitting the Spam`}</h2>\n    <pre><code parentName=\"pre\" {...{}}>{`# The spam filter that has been 'learned' for you\n> spam_classifier <- function(x){\n    prediction <- rep(NA, length(x)) # initialize prediction vector\n    prediction[x > 4] <- 1\n    prediction[x >= 3 & x <= 4] <- 0\n    prediction[x >= 2.2 & x < 3] <- 1\n    prediction[x >= 1.4 & x < 2.2] <- 0\n    prediction[x > 1.25 & x < 1.4] <- 1\n    prediction[x <= 1.25] <- 0\n    return(factor(prediction, levels = c(\"1\", \"0\"))) # prediction is either 0 or 1\n  }\n>\n# Apply spam_classifier to emails_full: pred_full\n> pred_full <- spam_classifier(emails_full$avg_capital_seq)\n>\n# Build confusion matrix for emails_full: conf_full\n> conf_full <- table(emails_full$spam, pred_full)\n>\n# Calculate the accuracy with conf_full: acc_full\n> acc_full <- sum(diag(conf_full))/sum(conf_full)\n>\n# Print acc_full\n> acc_full\n[1] 0.6561617\n`}</code></pre>\n    <p>{`This hard-coded classifier gave you an accuracy of around 65% on the full dataset, which is way worse than the 100% you had on the small dataset back in chapter 1. Hence, the classifier does not generalize well at all!`}</p>\n    <p>{`It's official now, the spamClassifier() from chapter 1 is bogus. It simply overfits on the emailsSmall set and, as a result, doesn't generalize to larger datasets such as emailsFull.`}</p>\n    <p>{`So let's try something else. On average, emails with a high frequency of sequential capital letters are spam. What if you simply filtered spam based on one threshold for avgCapitalSeq?`}</p>\n    <p>{`For example, you could filter all emails with avgCapitalSeq > 4 as spam. By doing this, you increase the interpretability of the classifier and restrict its complexity. However, this increases the bias, i.e. the error due to restricting your model.`}</p>\n    <p>{`Your job is to simplify the rules of spamClassifier and calculate the accuracy for the full set emailsFull. Next, compare it to that of the small set emailsSmall, which is coded for you. Does the model generalize now?`}</p>\n    <pre><code parentName=\"pre\" {...{}}>{`# The all-knowing classifier that has been learned for you\n# You should change the code of the classifier, simplifying it\n> spam_classifier <- function(x){\n    prediction <- rep(NA, length(x))\n    prediction[x > 4] <- 1\n    prediction[x <= 4] <- 0\n    return(factor(prediction, levels = c(\"1\", \"0\")))\n  }\n>\n# conf_small and acc_small have been calculated for you\n> conf_small <- table(emails_small$spam, spam_classifier(emails_small$avg_capital_seq))\n> acc_small <- sum(diag(conf_small)) / sum(conf_small)\n> acc_small\n[1] 0.7692308\n>\n# Apply spam_classifier to emails_full and calculate the confusion matrix: conf_full\n> conf_full <- table(emails_full$spam, spam_classifier(emails_full$avg_capital_seq))\n>\n# Calculate acc_full\n> acc_full <- sum(diag(conf_full)) / sum(conf_full)\n>\n# Print acc_full\n> acc_full\n[1] 0.7259291\n`}</code></pre>\n    <p>{`The model no longer fits the small dataset perfectly but it fits the big dataset better. You increased the bias on the model and caused it to generalize better over the complete dataset. While the first classifier overfits the data, an accuracy of 73% is far from satisfying for a spam filter.`}</p>\n    <hr></hr>\n    <h2 {...{\n      \"id\": \"classification\"\n    }}>{`Classification`}</h2>\n    <p>{`The task of automatically classifying fields given features.`}</p>\n    <p><inlineCode parentName=\"p\">{`Observation`}</inlineCode>{`: vector of features, with a class.`}</p>\n    <p>{`The classification model will automatically assign a class based on previous observations.`}</p>\n    <p><inlineCode parentName=\"p\">{`Binary classification`}</inlineCode>{`: Two classes.`}</p>\n    <p><inlineCode parentName=\"p\">{`Multiclass classification`}</inlineCode>{`: More than two classes.`}</p>\n    <p><strong parentName=\"p\">{`Example`}</strong></p>\n    <ul>\n      <li parentName=\"ul\">{`a dataset consisting of persons`}</li>\n      <li parentName=\"ul\">{`features: age, weight and income`}</li>\n      <li parentName=\"ul\">{`class: - binary: happy or not happy - multiclass: happy, satisfied, not happy`}</li>\n      <li parentName=\"ul\">{`features can be numerical - height - age`}</li>\n      <li parentName=\"ul\">{`features can be categorical - travel class`}</li>\n    </ul>\n    <p><strong parentName=\"p\">{`Decision Trees`}</strong></p>\n    <p>{`Suppose you want a patient as sick or not sick (1 or 0).`}</p>\n    <p>{`Best task would be to start asking some questions.`}</p>\n    <p>{`Eg. are they young or old?\nIf old, have you smoked more than 10 years?\nIf young, is the patient vaccinated against measles?`}</p>\n    <p>{`These questions will begin to form a tree.`}</p>\n    <p>{`The tree consists of nodes and edges.`}</p>\n    <p>{`The start of the tree is the roots and the ends are the leafs.`}</p>\n    <p>{`There is also a parent-child relation.`}</p>\n    <p>{`The questions on the tree are simply queries about the features.`}</p>\n    <p><strong parentName=\"p\">{`Categorical feature`}</strong></p>\n    <ul>\n      <li parentName=\"ul\">{`Can be a feature test on itself`}</li>\n      <li parentName=\"ul\">{`travelClass: coach, business or first`}</li>\n    </ul>\n    <p><strong parentName=\"p\">{`Learn a tree`}</strong></p>\n    <ul>\n      <li parentName=\"ul\">{`use a training set`}</li>\n      <li parentName=\"ul\">{`come up with queries (feature tests) at each node`}</li>\n      <li parentName=\"ul\">{`at each node - iterate over different feature tests - choose the best one`}</li>\n      <li parentName=\"ul\">{`comes down to two parts - make a list - choose the best one`}</li>\n    </ul>\n    <p><strong parentName=\"p\">{`Construct list of tests`}</strong></p>\n    <ul>\n      <li parentName=\"ul\">{`categorical - people/categories who haven't used the test yet`}</li>\n      <li parentName=\"ul\">{`numerical - choose feature - choose threshold for split`}</li>\n      <li parentName=\"ul\">{`choose best feature test - more complex - use splitting criteria to decide which is the best to use - `}<inlineCode parentName=\"li\">{`information gain`}</inlineCode>{` - entropy`}</li>\n    </ul>\n    <p><strong parentName=\"p\">{`Information Gain`}</strong></p>\n    <p>{`Defines how much info you gain about your training instances when you perform the split based on the feature test.`}</p>\n    <p>{`If tests lead to nicely divided classees -> high information gain.\nIf tests lead to scrambled classes -> low information gain.`}</p>\n    <p>{`Choose the test with the best information gain.`}</p>\n    <p><strong parentName=\"p\">{`Pruning`}</strong></p>\n    <ul>\n      <li parentName=\"ul\">{`number of nodes influences the chance of overfit.`}</li>\n      <li parentName=\"ul\">{`restrict size - higher bias - decreases the chance of an overfit`}</li>\n    </ul>\n    <h2 {...{\n      \"id\": \"learn-a-decision-tree\"\n    }}>{`Learn a Decision Tree`}</h2>\n    <p>{`To test your classification skills, you can build a decision tree that uses a person's age, gender, and travel class to predict whether or not they survived the Titanic. The titanic data frame has already been divided into training and test sets (named train and test).`}</p>\n    <p>{`In this exercise, you'll need train to build a decision tree. You can use the rpart() function of the rpart package for this. Behind the scenes, it performs the steps that Vincent explained in the video: coming up with possible feature tests and building a tree with the best of these tests.`}</p>\n    <p>{`Finally, a fancy plot can help you interpret the tree. You will need the rattle, rpart.plot, and RColorBrewer packages to display this.`}</p>\n    <pre><code parentName=\"pre\" {...{}}>{`# The train and test set are loaded into your workspace.\n>\n# Set random seed. Don't remove this line\n> set.seed(1)\n>\n# Load the rpart, rattle, rpart.plot and RColorBrewer package\n> library(rpart)\n> library(rattle)\n> library(rpart.plot)\n> library(RColorBrewer)\n>\n# Fill in the ___, build a tree model: tree\n> tree <- rpart(Survived ~ ., train, method=\"class\")\n>\n# Draw the decision tree - this in the console generates the decision tree\n> fancyRpartPlot(tree)\n`}</code></pre>\n    <p>{`Remember how Vincent told you that a tree is learned by separating the training set step-by-step? In an ideal world, the separations lead to subsets that all have the same class. In reality, however, each division will contain both positive and negative training observations. In this node, 76% of the training instances are positive and 24% are negative. The majority class thus is positive, or 1, which is signaled by the number 1 on top. The 36% bit tells you which percentage of the entire training set passes through this particular node. On each tree level, these percentages thus sum up to 100%. Finally, the Pclass = 1,2 bit specifies the feature test on which this node will be separated next. If the test comes out positive, the left branch is taken; if it's negative, the right branch is taken.`}</p>\n    <h2 {...{\n      \"id\": \"classify-with-the-decision-tree\"\n    }}>{`Classify with the Decision Tree`}</h2>\n    <p>{`The previous learning step involved proposing different tests on which to split nodes and then to select the best tests using an appropriate splitting criterion. You were spared from all the implementation hassles that come with that: the rpart() function did all of that for you.`}</p>\n    <p>{`Now you are going to classify the instances that are in the test set. As before, the data frames titanic, train and test are available in your workspace. You'll only want to work with the test set, though.`}</p>\n    <pre><code parentName=\"pre\" {...{}}>{`# The train and test set are loaded into your workspace.\n>\n# Code from previous exercise\n> set.seed(1)\n> library(rpart)\n> tree <- rpart(Survived ~ ., train, method = \"class\")\n>\n# Predict the values of the test set: pred\n> pred <- predict(tree, test, type=\"class\")\n>\n# Construct the confusion matrix: conf\n> conf <- table(test$Survived, pred)\n>\n# Print out the accuracy\n> sum(diag(conf)) / sum(conf)\n[1] 0.7990654\n`}</code></pre>\n    <p>{`Looking good! What does the accuracy tell you? Around 80 percent of all test instances have been classified correctly. That's not bad!`}</p>\n    <h2 {...{\n      \"id\": \"pruning-the-tree\"\n    }}>{`Pruning the Tree`}</h2>\n    <pre><code parentName=\"pre\" {...{}}>{`# Calculation of a complex tree\n> set.seed(1)\n> tree <- rpart(Survived ~ ., train, method = \"class\", control = rpart.control(cp=0.00001))\n>\n# Draw the complex tree\n> fancyRpartPlot(tree)\n>\n# Prune the tree: pruned\n> pruned <- prune(tree, cp=0.01)\n>\n# Draw pruned\n> fancyRpartPlot(pruned)\n`}</code></pre>\n    <p>{`Another way to check if you overfit your model is by comparing the accuracy on the training set with the accuracy on the test set. You'd see that the difference between those two is smaller for the simpler tree. You can also set the `}<inlineCode parentName=\"p\">{`cp`}</inlineCode>{` argument while learning the tree with `}<inlineCode parentName=\"p\">{`rpart()`}</inlineCode>{` using `}<inlineCode parentName=\"p\">{`rpart.control`}</inlineCode>{`.`}</p>\n    <h2 {...{\n      \"id\": \"gini-criterion\"\n    }}>{`Gini Criterion`}</h2>\n    <p><inlineCode parentName=\"p\">{`rpart`}</inlineCode>{` by default uses the `}<inlineCode parentName=\"p\">{`Gini Criterion`}</inlineCode>{` for making decision trees.`}</p>\n    <pre><code parentName=\"pre\" {...{}}>{`# Set random seed. Don't remove this line.\n> set.seed(1)\n>\n# Train and test tree with gini criterion\n> tree_g <- rpart(spam ~ ., train, method = \"class\")\n> pred_g <- predict(tree_g, test, type = \"class\")\n> conf_g <- table(test$spam, pred_g)\n> acc_g <- sum(diag(conf_g)) / sum(conf_g)\n>\n# Change the first line of code to use information gain as splitting criterion\n> tree_i <- rpart(spam ~ ., train, method = \"class\", parms = list(split = \"information\"))\n> pred_i <- predict(tree_i, test, type = \"class\")\n> conf_i <- table(test$spam, pred_i)\n> acc_i <- sum(diag(conf_i)) / sum(conf_i)\n>\n# Draw a fancy plot of both tree_g and tree_i\n> fancyRpartPlot(tree_g)\n> fancyRpartPlot(tree_i)\n>\n>\n# Print out acc_g and acc_i\n> acc_i\n[1] 0.8963768\n> acc_g\n[1] 0.8905797\n`}</code></pre>\n    <h2 {...{\n      \"id\": \"k-nearest-neighbors\"\n    }}>{`k-Nearest Neighbors`}</h2>\n    <p>{`Getting acquinted with instance based learning.`}</p>\n    <ul>\n      <li parentName=\"ul\">{`Save training set in memory`}</li>\n      <li parentName=\"ul\">{`No real model like `}<inlineCode parentName=\"li\">{`decision tree`}</inlineCode></li>\n      <li parentName=\"ul\"><inlineCode parentName=\"li\">{`Compare`}</inlineCode>{` unseen instances to training set`}</li>\n      <li parentName=\"ul\"><inlineCode parentName=\"li\">{`Predict`}</inlineCode>{` using the `}<inlineCode parentName=\"li\">{`comparison`}</inlineCode>{` of `}<inlineCode parentName=\"li\">{`unseen data`}</inlineCode>{` and the `}<inlineCode parentName=\"li\">{`training set`}</inlineCode></li>\n    </ul>\n    <p>{`k-Nearest Neighbour example`}</p>\n    <p>{`2 features: x1, x2`}</p>\n    <ul>\n      <li parentName=\"ul\">{`all have red or blue class - binary classification problem`}</li>\n    </ul>\n    <p>{`This will save the complete training set`}</p>\n    <p>{`Given unseen observation with features, it will compare the new features with the old training set. It will find the closest observation and assign the same class.`}</p>\n    <p>{`This is essentially a Euclidian distance measurement.`}</p>\n    <p>{`That is for k = 1.`}</p>\n    <p>{`If k = 5, it will use the 5 most similar observations (neighbours).`}</p>\n    <p>{`Distance metric is important. We can use the standard Euclidian Distance. We can also use the Manhattan distance:`}</p>\n    <p>{`Euclidian Distance: `}<inlineCode parentName=\"p\">{`sqr(sum((a[i]-b[i])**2))`}</inlineCode>{`\nManhattan Distance: `}<inlineCode parentName=\"p\">{`sum(abs(a[i] - b[i]))`}</inlineCode></p>\n    <h2 {...{\n      \"id\": \"scaling-example\"\n    }}>{`Scaling Example`}</h2>\n    <ul>\n      <li parentName=\"ul\">{`Dataset with`}<ul parentName=\"li\">\n          <li parentName=\"ul\">{`2 features: weight and height`}</li>\n          <li parentName=\"ul\">{`3 observations`}</li>\n        </ul></li>\n    </ul>\n    <ol>\n      <li parentName=\"ol\">{`Normalize all features - eg rescale values between 0 and 1`}</li>\n    </ol>\n    <ul>\n      <li parentName=\"ul\">{`this gives a better measurement between the distances`}</li>\n      <li parentName=\"ul\">{`don't forget to scale the new observations accordingly`}</li>\n    </ul>\n    <ol {...{\n      \"start\": 2\n    }}>\n      <li parentName=\"ol\">{`Categorical features`}</li>\n    </ol>\n    <ul>\n      <li parentName=\"ul\">{`How to use in distance metric?`}</li>\n      <li parentName=\"ul\">{`Use `}<inlineCode parentName=\"li\">{`dummy`}</inlineCode>{` variables`}</li>\n      <li parentName=\"ul\">{`eg mother tongue: Spanish, Italian or French.`}<ul parentName=\"li\">\n          <li parentName=\"ul\">{`create new features with possible 1 or 0`}</li>\n        </ul></li>\n    </ul>\n    <pre><code parentName=\"pre\" {...{}}>{`> train_labels <- train$Survived\n> test_labels <- test$Survived\n>\n# Copy train and test to knn_train and knn_test\n> knn_train <- train\n> knn_test <- test\n>\n# Drop Survived column for knn_train and knn_test\n> knn_train$Survived <- NULL\n> knn_test$Survived <- NULL\n>\n# Normalize Pclass\n> min_class <- min(knn_train$Pclass)\n> max_class <- max(knn_train$Pclass)\n> knn_train$Pclass <- (knn_train$Pclass - min_class) / (max_class - min_class)\n> knn_test$Pclass <- (knn_test$Pclass - min_class) / (max_class - min_class)\n>\n# Normalize Age\n> min_age <- min(knn_train$Age)\n> max_age <- max(knn_train$Age)\n> knn_train$Age <- (knn_train$Age - min_age) / (max_age - min_age)\n> knn_test$Age <- (knn_test$Age - min_age) / (max_age - min_age)\n`}</code></pre>\n    <pre><code parentName=\"pre\" {...{}}>{`# knn_train, knn_test, train_labels and test_labels are pre-loaded\n>\n# Set random seed. Don't remove this line.\n> set.seed(1)\n>\n# Load the class package\n> library(class)\n>\n# Fill in the ___, make predictions using knn: pred\n> pred <- knn(train = knn_train, test = knn_test, cl = train_labels, k = 5)\n>\n# Construct the confusion matrix: conf\n> conf <- table(test_labels, pred)\n>\n# Print out the confusion matrix\n> conf\n           pred\ntest_labels   1   0\n          1  61  24\n          0  17 112\n`}</code></pre>\n    <pre><code parentName=\"pre\" {...{}}>{`# knn_train, knn_test, train_labels and test_labels are pre-loaded\n>\n# Set random seed. Don't remove this line.\n> set.seed(1)\n>\n# Load the class package, define range and accs\n> library(class)\n> range <- 1:round(0.2 * nrow(knn_train))\n> accs <- rep(0, length(range))\n>\n> for (k in range) {\n\n    # Fill in the ___, make predictions using knn: pred\n    pred <- knn(train = knn_train, test = knn_test, cl = train_labels, k = k)\n\n    # Fill in the ___, construct the confusion matrix: conf\n    conf <- table(test_labels, pred)\n\n    # Fill in the ___, calculate the accuracy and store it in accs[k]\n    accs[k] <- sum(diag(conf)/sum(conf))\n  }\n>\n# Plot the accuracies. Title of x-axis is \"k\".\n> plot(range, accs, xlab = \"k\")\n>\n# Calculate the best k\n> which.max(accs)\n[1] 73\n`}</code></pre>\n    <h2 {...{\n      \"id\": \"interpreting-a-voronoi-diagram\"\n    }}>{`Interpreting a Voronoi Diagram`}</h2>\n    <p>{`A cool way to visualize how 1-Nearest Neighbor works with two-dimensional features is the Voronoi Diagram. It's basically a plot of all the training instances, together with a set of tiles around the points. This tile represents the region of influence of each point. When you want to classify a new observation, it will receive the class of the tile in which the coordinates fall. Pretty cool, right?`}</p>\n    <p>{`In the plot on the right you can see training instances that belong to either the blue or the red class. Each instance has two features: xx and yy. The top left instance, for example, has an xx value of around 0.05 and a yy value of 0.9.`}</p>\n\n    </MDXLayout>;\n}\n;\nMDXContent.isMDXComponent = true;\n      "],"sourceRoot":""}