{"expireTime":9007200849425533000,"key":"gatsby-plugin-mdx-entire-payload-ed12721cdc5a9b58f05fb6a86baf0a83-","val":{"mdast":{"type":"root","children":[{"type":"import","value":"import DefaultLayout from \"/Users/dennis.okeeffe/Project-Imposter/developer-notes/node_modules/gatsby-theme-docz/src/base/Layout.js\"","position":{"start":{"line":3,"column":1,"offset":2},"end":{"line":3,"column":133,"offset":134},"indent":[]}},{"type":"export","default":true,"value":"export default DefaultLayout","position":{"start":{"line":5,"column":1,"offset":136},"end":{"line":5,"column":29,"offset":164},"indent":[]}},{"type":"heading","depth":1,"children":[{"type":"text","value":"Random Forest Regression","position":{"start":{"line":7,"column":3,"offset":168},"end":{"line":7,"column":27,"offset":192},"indent":[]}}],"position":{"start":{"line":7,"column":1,"offset":166},"end":{"line":7,"column":27,"offset":192},"indent":[]}},{"type":"heading","depth":2,"children":[{"type":"text","value":"Intuition","position":{"start":{"line":9,"column":4,"offset":197},"end":{"line":9,"column":13,"offset":206},"indent":[]}}],"position":{"start":{"line":9,"column":1,"offset":194},"end":{"line":9,"column":13,"offset":206},"indent":[]}},{"type":"paragraph","children":[{"type":"text","value":"Random forest is a version of ensemble learning.","position":{"start":{"line":11,"column":1,"offset":208},"end":{"line":11,"column":49,"offset":256},"indent":[]}}],"position":{"start":{"line":11,"column":1,"offset":208},"end":{"line":11,"column":49,"offset":256},"indent":[]}},{"type":"paragraph","children":[{"type":"text","value":"It's when you take the same algorithm multiple times and create something more powerful.","position":{"start":{"line":13,"column":1,"offset":258},"end":{"line":13,"column":89,"offset":346},"indent":[]}}],"position":{"start":{"line":13,"column":1,"offset":258},"end":{"line":13,"column":89,"offset":346},"indent":[]}},{"type":"paragraph","children":[{"type":"strong","children":[{"type":"text","value":"Steps","position":{"start":{"line":15,"column":3,"offset":350},"end":{"line":15,"column":8,"offset":355},"indent":[]}}],"position":{"start":{"line":15,"column":1,"offset":348},"end":{"line":15,"column":10,"offset":357},"indent":[]}}],"position":{"start":{"line":15,"column":1,"offset":348},"end":{"line":15,"column":10,"offset":357},"indent":[]}},{"type":"list","ordered":true,"start":1,"spread":false,"children":[{"type":"listItem","spread":false,"checked":null,"children":[{"type":"paragraph","children":[{"type":"text","value":"Pick at random K data points from the Training Set.","position":{"start":{"line":17,"column":4,"offset":362},"end":{"line":17,"column":55,"offset":413},"indent":[]}}],"position":{"start":{"line":17,"column":4,"offset":362},"end":{"line":17,"column":55,"offset":413},"indent":[]}}],"position":{"start":{"line":17,"column":1,"offset":359},"end":{"line":17,"column":55,"offset":413},"indent":[]}},{"type":"listItem","spread":false,"checked":null,"children":[{"type":"paragraph","children":[{"type":"text","value":"Build the Decision Tree associated to these K data points.","position":{"start":{"line":18,"column":4,"offset":417},"end":{"line":18,"column":62,"offset":475},"indent":[]}}],"position":{"start":{"line":18,"column":4,"offset":417},"end":{"line":18,"column":62,"offset":475},"indent":[]}}],"position":{"start":{"line":18,"column":1,"offset":414},"end":{"line":18,"column":62,"offset":475},"indent":[]}},{"type":"listItem","spread":false,"checked":null,"children":[{"type":"paragraph","children":[{"type":"text","value":"Choose the number Ntree of trees you want to build and repeat steps 1 and 2.","position":{"start":{"line":19,"column":4,"offset":479},"end":{"line":19,"column":80,"offset":555},"indent":[]}}],"position":{"start":{"line":19,"column":4,"offset":479},"end":{"line":19,"column":80,"offset":555},"indent":[]}}],"position":{"start":{"line":19,"column":1,"offset":476},"end":{"line":19,"column":80,"offset":555},"indent":[]}},{"type":"listItem","spread":false,"checked":null,"children":[{"type":"paragraph","children":[{"type":"text","value":"For a new data point, make each one of your Ntree trees predict the value of ","position":{"start":{"line":20,"column":4,"offset":559},"end":{"line":20,"column":81,"offset":636},"indent":[]}},{"type":"inlineCode","value":"Y","position":{"start":{"line":20,"column":81,"offset":636},"end":{"line":20,"column":84,"offset":639},"indent":[]}},{"type":"text","value":" for the data point in question, and assign the new data point the average across all the predicted ","position":{"start":{"line":20,"column":84,"offset":639},"end":{"line":20,"column":184,"offset":739},"indent":[]}},{"type":"inlineCode","value":"Y","position":{"start":{"line":20,"column":184,"offset":739},"end":{"line":20,"column":187,"offset":742},"indent":[]}},{"type":"text","value":" values.","position":{"start":{"line":20,"column":187,"offset":742},"end":{"line":20,"column":195,"offset":750},"indent":[]}}],"position":{"start":{"line":20,"column":4,"offset":559},"end":{"line":20,"column":195,"offset":750},"indent":[]}}],"position":{"start":{"line":20,"column":1,"offset":556},"end":{"line":20,"column":195,"offset":750},"indent":[]}}],"position":{"start":{"line":17,"column":1,"offset":359},"end":{"line":20,"column":195,"offset":750},"indent":[1,1,1]}},{"type":"paragraph","children":[{"type":"text","value":"Doing this allows you to improve the accuracy of your prediction.","position":{"start":{"line":22,"column":1,"offset":752},"end":{"line":22,"column":66,"offset":817},"indent":[]}}],"position":{"start":{"line":22,"column":1,"offset":752},"end":{"line":22,"column":66,"offset":817},"indent":[]}},{"type":"paragraph","children":[{"type":"strong","children":[{"type":"text","value":"Example","position":{"start":{"line":24,"column":3,"offset":821},"end":{"line":24,"column":10,"offset":828},"indent":[]}}],"position":{"start":{"line":24,"column":1,"offset":819},"end":{"line":24,"column":12,"offset":830},"indent":[]}}],"position":{"start":{"line":24,"column":1,"offset":819},"end":{"line":24,"column":12,"offset":830},"indent":[]}},{"type":"paragraph","children":[{"type":"text","value":"How many lollies in a jar? Imagine taking notes of every guess - getting around 1000 and then beginning to average them out or take the median. Statistically speaking, you have a highly likelihood of being closer to the truth.","position":{"start":{"line":26,"column":1,"offset":832},"end":{"line":26,"column":227,"offset":1058},"indent":[]}}],"position":{"start":{"line":26,"column":1,"offset":832},"end":{"line":26,"column":227,"offset":1058},"indent":[]}},{"type":"paragraph","children":[{"type":"text","value":"Once you hit the middle of the normal distribution, you are more likely to be on the money for the guess.","position":{"start":{"line":28,"column":1,"offset":1060},"end":{"line":28,"column":106,"offset":1165},"indent":[]}}],"position":{"start":{"line":28,"column":1,"offset":1060},"end":{"line":28,"column":106,"offset":1165},"indent":[]}},{"type":"heading","depth":2,"children":[{"type":"text","value":"PYTHON","position":{"start":{"line":30,"column":4,"offset":1170},"end":{"line":30,"column":10,"offset":1176},"indent":[]}}],"position":{"start":{"line":30,"column":1,"offset":1167},"end":{"line":30,"column":10,"offset":1176},"indent":[]}},{"type":"paragraph","children":[{"type":"text","value":"This is the last regression model. If you understand decision tree regression, you'll understand random forest.","position":{"start":{"line":32,"column":1,"offset":1178},"end":{"line":32,"column":112,"offset":1289},"indent":[]}}],"position":{"start":{"line":32,"column":1,"offset":1178},"end":{"line":32,"column":112,"offset":1289},"indent":[]}},{"type":"paragraph","children":[{"type":"text","value":"From decision tree, we know that we will need the visualisation using the non-continuous result.","position":{"start":{"line":34,"column":1,"offset":1291},"end":{"line":34,"column":97,"offset":1387},"indent":[]}}],"position":{"start":{"line":34,"column":1,"offset":1291},"end":{"line":34,"column":97,"offset":1387},"indent":[]}},{"type":"paragraph","children":[{"type":"text","value":"For the regressor, we use RandomForestRegressor library.","position":{"start":{"line":36,"column":1,"offset":1389},"end":{"line":36,"column":57,"offset":1445},"indent":[]}}],"position":{"start":{"line":36,"column":1,"offset":1389},"end":{"line":36,"column":57,"offset":1445},"indent":[]}},{"type":"code","lang":"python","meta":null,"value":"# Prediciting the Random Forest results\n# Create the Regressor\nfrom sklearn.ensemble import RandomForestRegressor\nregressor = RandomForestRegressor(random_state=0)\nregressor.fit(X, y)","position":{"start":{"line":38,"column":1,"offset":1447},"end":{"line":44,"column":4,"offset":1644},"indent":[1,1,1,1,1,1]}},{"type":"paragraph","children":[{"type":"text","value":"Simply, with these lines, we can already determine that the graph is no longer continuous.","position":{"start":{"line":46,"column":1,"offset":1646},"end":{"line":46,"column":91,"offset":1736},"indent":[]}}],"position":{"start":{"line":46,"column":1,"offset":1646},"end":{"line":46,"column":91,"offset":1736},"indent":[]}},{"type":"paragraph","children":[{"type":"text","value":"By having several decision trees, we end up with a lot more \"steps\" than we had with just one decision tree.","position":{"start":{"line":48,"column":1,"offset":1738},"end":{"line":48,"column":109,"offset":1846},"indent":[]}}],"position":{"start":{"line":48,"column":1,"offset":1738},"end":{"line":48,"column":109,"offset":1846},"indent":[]}},{"type":"paragraph","children":[{"type":"text","value":"More tree !== more steps. The more trees you have, the more the average will converge towards the same average.","position":{"start":{"line":50,"column":1,"offset":1848},"end":{"line":50,"column":112,"offset":1959},"indent":[]}}],"position":{"start":{"line":50,"column":1,"offset":1848},"end":{"line":50,"column":112,"offset":1959},"indent":[]}},{"type":"paragraph","children":[{"type":"text","value":"Generally the steps will become better placed depending on the average.","position":{"start":{"line":52,"column":1,"offset":1961},"end":{"line":52,"column":72,"offset":2032},"indent":[]}}],"position":{"start":{"line":52,"column":1,"offset":1961},"end":{"line":52,"column":72,"offset":2032},"indent":[]}},{"type":"export","value":"export const _frontmatter = {}","position":{"start":{"line":55,"column":1,"offset":2035},"end":{"line":55,"column":31,"offset":2065},"indent":[]}}],"position":{"start":{"line":1,"column":1,"offset":0},"end":{"line":55,"column":31,"offset":2065}}},"scopeImports":[],"scopeIdentifiers":[],"rawMDXOutput":"/* @jsx mdx */\nimport { mdx } from '@mdx-js/react';\n/* @jsx mdx */\nimport DefaultLayout from \"/Users/dennis.okeeffe/Project-Imposter/developer-notes/node_modules/gatsby-theme-docz/src/base/Layout.js\"\nexport const _frontmatter = {};\nconst makeShortcode = name => function MDXDefaultShortcode(props) {\n  console.warn(\"Component \" + name + \" was not imported, exported, or provided by MDXProvider as global scope\")\n  return <div {...props}/>\n};\n\nconst layoutProps = {\n  _frontmatter\n};\nconst MDXLayout = DefaultLayout\nexport default function MDXContent({\n  components,\n  ...props\n}) {\n  return <MDXLayout {...layoutProps} {...props} components={components} mdxType=\"MDXLayout\">\n\n\n    <h1 {...{\n      \"id\": \"random-forest-regression\"\n    }}>{`Random Forest Regression`}</h1>\n    <h2 {...{\n      \"id\": \"intuition\"\n    }}>{`Intuition`}</h2>\n    <p>{`Random forest is a version of ensemble learning.`}</p>\n    <p>{`It's when you take the same algorithm multiple times and create something more powerful.`}</p>\n    <p><strong parentName=\"p\">{`Steps`}</strong></p>\n    <ol>\n      <li parentName=\"ol\">{`Pick at random K data points from the Training Set.`}</li>\n      <li parentName=\"ol\">{`Build the Decision Tree associated to these K data points.`}</li>\n      <li parentName=\"ol\">{`Choose the number Ntree of trees you want to build and repeat steps 1 and 2.`}</li>\n      <li parentName=\"ol\">{`For a new data point, make each one of your Ntree trees predict the value of `}<inlineCode parentName=\"li\">{`Y`}</inlineCode>{` for the data point in question, and assign the new data point the average across all the predicted `}<inlineCode parentName=\"li\">{`Y`}</inlineCode>{` values.`}</li>\n    </ol>\n    <p>{`Doing this allows you to improve the accuracy of your prediction.`}</p>\n    <p><strong parentName=\"p\">{`Example`}</strong></p>\n    <p>{`How many lollies in a jar? Imagine taking notes of every guess - getting around 1000 and then beginning to average them out or take the median. Statistically speaking, you have a highly likelihood of being closer to the truth.`}</p>\n    <p>{`Once you hit the middle of the normal distribution, you are more likely to be on the money for the guess.`}</p>\n    <h2 {...{\n      \"id\": \"python\"\n    }}>{`PYTHON`}</h2>\n    <p>{`This is the last regression model. If you understand decision tree regression, you'll understand random forest.`}</p>\n    <p>{`From decision tree, we know that we will need the visualisation using the non-continuous result.`}</p>\n    <p>{`For the regressor, we use RandomForestRegressor library.`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-python\"\n      }}>{`# Prediciting the Random Forest results\n# Create the Regressor\nfrom sklearn.ensemble import RandomForestRegressor\nregressor = RandomForestRegressor(random_state=0)\nregressor.fit(X, y)\n`}</code></pre>\n    <p>{`Simply, with these lines, we can already determine that the graph is no longer continuous.`}</p>\n    <p>{`By having several decision trees, we end up with a lot more \"steps\" than we had with just one decision tree.`}</p>\n    <p>{`More tree !== more steps. The more trees you have, the more the average will converge towards the same average.`}</p>\n    <p>{`Generally the steps will become better placed depending on the average.`}</p>\n\n    </MDXLayout>;\n}\n\n;\nMDXContent.isMDXComponent = true;"}}