{"expireTime":9007200852878373000,"key":"gatsby-plugin-mdx-entire-payload-cf9a74bf7397e26d31d554c75772af3e-","val":{"mdast":{"type":"root","children":[{"type":"import","value":"import DefaultLayout from \"/Users/dennis.okeeffe/Project-Imposter/developer-notes/node_modules/gatsby-theme-docz/src/base/Layout.js\"","position":{"start":{"line":3,"column":1,"offset":2},"end":{"line":3,"column":133,"offset":134},"indent":[]}},{"type":"export","default":true,"value":"export default DefaultLayout","position":{"start":{"line":5,"column":1,"offset":136},"end":{"line":5,"column":29,"offset":164},"indent":[]}},{"type":"heading","depth":1,"children":[{"type":"text","value":"Python Examples","position":{"start":{"line":8,"column":3,"offset":169},"end":{"line":8,"column":18,"offset":184},"indent":[]}}],"position":{"start":{"line":8,"column":1,"offset":167},"end":{"line":8,"column":18,"offset":184},"indent":[]}},{"type":"paragraph","children":[{"type":"text","value":"This section is just a collecton of interesting real world scripts that were used for one reason for another.","position":{"start":{"line":10,"column":1,"offset":186},"end":{"line":10,"column":110,"offset":295},"indent":[]}}],"position":{"start":{"line":10,"column":1,"offset":186},"end":{"line":10,"column":110,"offset":295},"indent":[]}},{"type":"heading","depth":2,"children":[{"type":"text","value":"Sentry Script w/ datetime, requests, csv, multiprocessing and threading","position":{"start":{"line":12,"column":4,"offset":300},"end":{"line":12,"column":75,"offset":371},"indent":[]}}],"position":{"start":{"line":12,"column":1,"offset":297},"end":{"line":12,"column":75,"offset":371},"indent":[]}},{"type":"paragraph","children":[{"type":"text","value":"A script used to fetch 92k events from Sentry using all OS cores and multithreading.","position":{"start":{"line":14,"column":1,"offset":373},"end":{"line":14,"column":85,"offset":457},"indent":[]}}],"position":{"start":{"line":14,"column":1,"offset":373},"end":{"line":14,"column":85,"offset":457},"indent":[]}},{"type":"code","lang":"python","meta":null,"value":"\"\"\"\nRead all the issue IDs saved, then iterate through, find all their issues and paginate through all\n\"\"\"\nimport requests\nimport csv\nimport os\nimport sys\nimport urllib.request\nimport datetime\n\nfrom multiprocessing import Pool\nimport threading\ncsv_writer_lock = threading.Lock()\ntotal_events_lock = threading.Lock()\nrecord_log = True\n\n# for logging\nif record_log:\n\told_stdout = sys.stdout\n\tlog_file = open(\"fetch_events_by_issue.log\",\"w\")\n\tsys.stdout = log_file\n\n\norganization_slug = \"REDACTED\"\nproject_slug = \"REDACTED\"\nbase_url = \"https://sentry.io/api/0\"\n\nheaders = {\n\t'Authorization': 'Bearer REDACTED'\n}\n\n\ncursor = 0\n\ndef post_to_slack(message):\n\tdata = '{\"text\":\"' + message + '\"}'\n\turl = 'https://hooks.slack.com/services/REDACTED/REDACTED/REDACTED'\n\treq = urllib.request.Request(url, data.encode('utf-8'), {'Content-Type': 'application/json'})\n\tres = urllib.request.urlopen(req)\n\n\n# not the best idea but whatever\ntotal_events_with_console_logs = 0\nbreadcrumbs = False\ndef handle_event(event):\n\ttry:\n\t\tglobal total_events_with_console_logs\n\t\tglobal breadcrumbs\n\n\t\tevent_has_console_breadcrumbs = False\n\t\t# get single event from the API\n\t\tevent_id = event.get(\"eventID\")\n\t\tissue_id = event.get(\"groupID\")\n\t\tprint(\"Found a single event with ID: \" + event_id)\n\t\tsingle_event_url = base_url + \"/projects/\" + organization_slug + \"/\" + project_slug + \"/events/\" + event_id + \"/\"\n\t\tevent_response = requests.get(single_event_url, headers=headers)\n\n\t\trow_dict = {\n\t\t\t'issue_id': issue_id,\n\t\t\t'event_id': event_id,\n\t\t\t'console_output': ''\n\t\t}\n\t\t# retreive the metadata\n\t\tprint(f'[{datetime.datetime.now()}]  Trying to find breadcrumbs for event ID: {event_id}')\n\t\tevent_data = event_response.json()\n\n\t\tfor entry in event_data['entries']:\n\t\t\tif entry['type'] == 'breadcrumbs':\n\t\t\t\tbreadcrumbs = entry.get('data', {}).get('values', {})\n\t\tif breadcrumbs and len(breadcrumbs) > 0:\n\t\t\tfor crumb in breadcrumbs:\n\t\t\t\tif crumb['category'] == 'console':\n\t\t\t\t\tevent_has_console_breadcrumbs = True\n\t\t\t\t\tprint(f'[{datetime.datetime.now()}] Found a console breadcrumb!')\n\t\t\t\t\trow_dict['console_output'] = crumb\n\t\t\t\t\twith csv_writer_lock:\n\t\t\t\t\t\twriter.writerow(row_dict)\n\n\t\tif event_has_console_breadcrumbs:\n\t\t\twith total_events_lock:\n\t\t\t\ttry:\n\t\t\t\t\ttotal_events_with_console_logs += 1\n\t\t\t\texcept Exception as e:\n\t\t\t\t\tprint(f'[{datetime.datetime.now()}] Failed to append to total_events_with_console_logs')\n\t\telse:\n\t\t\tprint(f'[{datetime.datetime.now()}]  Did not find any breadcrumbs.')\n\t\t\trow_dict['console_output'] = 'No console output!'\n\t\t\twith csv_writer_lock:\n\t\t\t\twriter.writerow(row_dict)\n\texcept Exception as e:\n\t\tprint(f'Failed for {event.get(\"eventID\")}', e.message)\n\n\ndef handle_issue(issue_id):\n\t\"\"\"\n\tFor issue ID, continually fetch all events and paginate until there are none.\n\n\tWith each event, handle it such that it fetches and appends the required data.\n\t\"\"\"\n\t# writer.writeheader()\n\tlist_events_url = f'{base_url}/issues/{issue_id}/events/'\n\tattempt = 1\n\twhile list_events_url is not None:\n\t\ttry:\n\t\t\tif attempt > 5:\n\t\t\t\tpost_to_slack(f'[{datetime.datetime.now()} FAILED]: Reached 5 attempts for {list_events_url}')\n\t\t\t\tlist_events_url = None\n\t\t\t\tbreak\n\t\t\t# get the events\n\t\t\tprint(f'[{datetime.datetime.now()}] NEXT] Getting page of results: {list_events_url}')\n\t\t\tresponse = requests.get(list_events_url, headers=headers)\n\t\t\t# data is an array of events\n\t\t\tdata = response.json()\n\t\t\tfor event in data:\n\t\t\t\thandle_event(event)\n\t\t\t# is there another page of events?\n\t\t\tlink = response.headers.get(\"Link\")\n\t\t\t# reset attempt if successful\n\t\t\tattempt = 1\n\t\t\tif link and 'rel=\"next\"; results=\"true\"' in link:\n\t\t\t\tpost_to_slack(f'[{datetime.datetime.now()} SUCCESS]: Finished results for {list_events_url}')\n\t\t\t\tlist_events_url = link.split()[4][1:-2]\n\t\t\t\tprint(f'[{datetime.datetime.now()} NEXT]: Getting another page of event from issue_id {issue_id} - URL {link}.')\n\t\t\telse:\n\t\t\t\tpost_to_slack(f'[{datetime.datetime.now()} SUCCESS]: Finished results for {list_events_url}')\n\t\t\t\tlist_events_url = None\n\t\texcept Exception as e:\n\t\t\tprint(f'[{datetime.datetime.now()} WARNING]Fetch attempt {attempt} failed: {list_events_url}', e)\n\t\t\tpost_to_slack\n\t\t\tattempt += 1\n\ntry:\n\twith open('./fetch_events_by_issue.csv', 'w') as output_file:\n\t\tfieldnames = ['issue_id', 'event_id', 'console_output']\n\t\twriter = csv.DictWriter(output_file, fieldnames=fieldnames)\n\t\tprint(f'[{datetime.datetime.now()}Writing headers for csv')\n\t\twith open('./project_issues.csv', 'r') as file:\n\t\t\tdata = file.read().splitlines()\n\t\t\t# use attempts to give up after trying 10 times on the same URL\n\t\t\tchunksize = 1\n\t\t\tproc_count = os.cpu_count()\n\t\t\twith Pool(processes=proc_count) as pool:\n\t\t\t\tresult = pool.map(handle_issue, data, chunksize)\n\n\nexcept:\n\tpost_to_slack(f'[{datetime.datetime.now()} FAILED]: Script crashed')\nfinally:\n\tpost_to_slack(f'[{datetime.datetime.now()} COMPLETED]: Script finished')\n\tif record_log:\n\t\tsys.stdout = old_stdout\n\t\tlog_file.close()","position":{"start":{"line":16,"column":1,"offset":459},"end":{"line":169,"column":4,"offset":5403},"indent":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]}},{"type":"export","value":"export const _frontmatter = {}","position":{"start":{"line":172,"column":1,"offset":5406},"end":{"line":172,"column":31,"offset":5436},"indent":[]}}],"position":{"start":{"line":1,"column":1,"offset":0},"end":{"line":172,"column":31,"offset":5436}}},"scopeImports":[],"scopeIdentifiers":[],"rawMDXOutput":"/* @jsx mdx */\nimport { mdx } from '@mdx-js/react';\n/* @jsx mdx */\nimport DefaultLayout from \"/Users/dennis.okeeffe/Project-Imposter/developer-notes/node_modules/gatsby-theme-docz/src/base/Layout.js\"\nexport const _frontmatter = {};\nconst makeShortcode = name => function MDXDefaultShortcode(props) {\n  console.warn(\"Component \" + name + \" was not imported, exported, or provided by MDXProvider as global scope\")\n  return <div {...props}/>\n};\n\nconst layoutProps = {\n  _frontmatter\n};\nconst MDXLayout = DefaultLayout\nexport default function MDXContent({\n  components,\n  ...props\n}) {\n  return <MDXLayout {...layoutProps} {...props} components={components} mdxType=\"MDXLayout\">\n\n\n    <h1 {...{\n      \"id\": \"python-examples\"\n    }}>{`Python Examples`}</h1>\n    <p>{`This section is just a collecton of interesting real world scripts that were used for one reason for another.`}</p>\n    <h2 {...{\n      \"id\": \"sentry-script-w-datetime-requests-csv-multiprocessing-and-threading\"\n    }}>{`Sentry Script w/ datetime, requests, csv, multiprocessing and threading`}</h2>\n    <p>{`A script used to fetch 92k events from Sentry using all OS cores and multithreading.`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-python\"\n      }}>{`\"\"\"\nRead all the issue IDs saved, then iterate through, find all their issues and paginate through all\n\"\"\"\nimport requests\nimport csv\nimport os\nimport sys\nimport urllib.request\nimport datetime\n\nfrom multiprocessing import Pool\nimport threading\ncsv_writer_lock = threading.Lock()\ntotal_events_lock = threading.Lock()\nrecord_log = True\n\n# for logging\nif record_log:\n    old_stdout = sys.stdout\n    log_file = open(\"fetch_events_by_issue.log\",\"w\")\n    sys.stdout = log_file\n\n\norganization_slug = \"REDACTED\"\nproject_slug = \"REDACTED\"\nbase_url = \"https://sentry.io/api/0\"\n\nheaders = {\n    'Authorization': 'Bearer REDACTED'\n}\n\n\ncursor = 0\n\ndef post_to_slack(message):\n    data = '{\"text\":\"' + message + '\"}'\n    url = 'https://hooks.slack.com/services/REDACTED/REDACTED/REDACTED'\n    req = urllib.request.Request(url, data.encode('utf-8'), {'Content-Type': 'application/json'})\n    res = urllib.request.urlopen(req)\n\n\n# not the best idea but whatever\ntotal_events_with_console_logs = 0\nbreadcrumbs = False\ndef handle_event(event):\n    try:\n        global total_events_with_console_logs\n        global breadcrumbs\n\n        event_has_console_breadcrumbs = False\n        # get single event from the API\n        event_id = event.get(\"eventID\")\n        issue_id = event.get(\"groupID\")\n        print(\"Found a single event with ID: \" + event_id)\n        single_event_url = base_url + \"/projects/\" + organization_slug + \"/\" + project_slug + \"/events/\" + event_id + \"/\"\n        event_response = requests.get(single_event_url, headers=headers)\n\n        row_dict = {\n            'issue_id': issue_id,\n            'event_id': event_id,\n            'console_output': ''\n        }\n        # retreive the metadata\n        print(f'[{datetime.datetime.now()}]  Trying to find breadcrumbs for event ID: {event_id}')\n        event_data = event_response.json()\n\n        for entry in event_data['entries']:\n            if entry['type'] == 'breadcrumbs':\n                breadcrumbs = entry.get('data', {}).get('values', {})\n        if breadcrumbs and len(breadcrumbs) > 0:\n            for crumb in breadcrumbs:\n                if crumb['category'] == 'console':\n                    event_has_console_breadcrumbs = True\n                    print(f'[{datetime.datetime.now()}] Found a console breadcrumb!')\n                    row_dict['console_output'] = crumb\n                    with csv_writer_lock:\n                        writer.writerow(row_dict)\n\n        if event_has_console_breadcrumbs:\n            with total_events_lock:\n                try:\n                    total_events_with_console_logs += 1\n                except Exception as e:\n                    print(f'[{datetime.datetime.now()}] Failed to append to total_events_with_console_logs')\n        else:\n            print(f'[{datetime.datetime.now()}]  Did not find any breadcrumbs.')\n            row_dict['console_output'] = 'No console output!'\n            with csv_writer_lock:\n                writer.writerow(row_dict)\n    except Exception as e:\n        print(f'Failed for {event.get(\"eventID\")}', e.message)\n\n\ndef handle_issue(issue_id):\n    \"\"\"\n    For issue ID, continually fetch all events and paginate until there are none.\n\n    With each event, handle it such that it fetches and appends the required data.\n    \"\"\"\n    # writer.writeheader()\n    list_events_url = f'{base_url}/issues/{issue_id}/events/'\n    attempt = 1\n    while list_events_url is not None:\n        try:\n            if attempt > 5:\n                post_to_slack(f'[{datetime.datetime.now()} FAILED]: Reached 5 attempts for {list_events_url}')\n                list_events_url = None\n                break\n            # get the events\n            print(f'[{datetime.datetime.now()}] NEXT] Getting page of results: {list_events_url}')\n            response = requests.get(list_events_url, headers=headers)\n            # data is an array of events\n            data = response.json()\n            for event in data:\n                handle_event(event)\n            # is there another page of events?\n            link = response.headers.get(\"Link\")\n            # reset attempt if successful\n            attempt = 1\n            if link and 'rel=\"next\"; results=\"true\"' in link:\n                post_to_slack(f'[{datetime.datetime.now()} SUCCESS]: Finished results for {list_events_url}')\n                list_events_url = link.split()[4][1:-2]\n                print(f'[{datetime.datetime.now()} NEXT]: Getting another page of event from issue_id {issue_id} - URL {link}.')\n            else:\n                post_to_slack(f'[{datetime.datetime.now()} SUCCESS]: Finished results for {list_events_url}')\n                list_events_url = None\n        except Exception as e:\n            print(f'[{datetime.datetime.now()} WARNING]Fetch attempt {attempt} failed: {list_events_url}', e)\n            post_to_slack\n            attempt += 1\n\ntry:\n    with open('./fetch_events_by_issue.csv', 'w') as output_file:\n        fieldnames = ['issue_id', 'event_id', 'console_output']\n        writer = csv.DictWriter(output_file, fieldnames=fieldnames)\n        print(f'[{datetime.datetime.now()}Writing headers for csv')\n        with open('./project_issues.csv', 'r') as file:\n            data = file.read().splitlines()\n            # use attempts to give up after trying 10 times on the same URL\n            chunksize = 1\n            proc_count = os.cpu_count()\n            with Pool(processes=proc_count) as pool:\n                result = pool.map(handle_issue, data, chunksize)\n\n\nexcept:\n    post_to_slack(f'[{datetime.datetime.now()} FAILED]: Script crashed')\nfinally:\n    post_to_slack(f'[{datetime.datetime.now()} COMPLETED]: Script finished')\n    if record_log:\n        sys.stdout = old_stdout\n        log_file.close()\n`}</code></pre>\n\n    </MDXLayout>;\n}\n\n;\nMDXContent.isMDXComponent = true;"}}