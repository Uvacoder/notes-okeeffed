{"expireTime":9007200849425595000,"key":"gatsby-plugin-mdx-entire-payload-71e5005b5aa71be5c60204b9d8af5984-","val":{"mdast":{"type":"root","children":[{"type":"import","value":"import DefaultLayout from \"/Users/dennis.okeeffe/Project-Imposter/developer-notes/node_modules/gatsby-theme-docz/src/base/Layout.js\"","position":{"start":{"line":3,"column":1,"offset":2},"end":{"line":3,"column":133,"offset":134},"indent":[]}},{"type":"export","default":true,"value":"export default DefaultLayout","position":{"start":{"line":5,"column":1,"offset":136},"end":{"line":5,"column":29,"offset":164},"indent":[]}},{"type":"heading","depth":1,"children":[{"type":"text","value":"Big Data Tools and Technologies","position":{"start":{"line":8,"column":3,"offset":169},"end":{"line":8,"column":34,"offset":200},"indent":[]}}],"position":{"start":{"line":8,"column":1,"offset":167},"end":{"line":8,"column":34,"offset":200},"indent":[]}},{"type":"heading","depth":2,"children":[{"type":"text","value":"Apache Hadoop","position":{"start":{"line":10,"column":4,"offset":205},"end":{"line":10,"column":17,"offset":218},"indent":[]}}],"position":{"start":{"line":10,"column":1,"offset":202},"end":{"line":10,"column":17,"offset":218},"indent":[]}},{"type":"paragraph","children":[{"type":"text","value":"Hadoop is a very significant player in the Big Data landscape.","position":{"start":{"line":12,"column":1,"offset":220},"end":{"line":12,"column":63,"offset":282},"indent":[]}}],"position":{"start":{"line":12,"column":1,"offset":220},"end":{"line":12,"column":63,"offset":282},"indent":[]}},{"type":"paragraph","children":[{"type":"text","value":"It's an open-sourced framework for distributed storage and processing of very large data sets.","position":{"start":{"line":14,"column":1,"offset":284},"end":{"line":14,"column":95,"offset":378},"indent":[]}}],"position":{"start":{"line":14,"column":1,"offset":284},"end":{"line":14,"column":95,"offset":378},"indent":[]}},{"type":"paragraph","children":[{"type":"text","value":"Originally built in 2005 by a Yahoo engineer.","position":{"start":{"line":16,"column":1,"offset":380},"end":{"line":16,"column":46,"offset":425},"indent":[]}}],"position":{"start":{"line":16,"column":1,"offset":380},"end":{"line":16,"column":46,"offset":425},"indent":[]}},{"type":"paragraph","children":[{"type":"text","value":"It was inspired by Google's ","position":{"start":{"line":18,"column":1,"offset":427},"end":{"line":18,"column":29,"offset":455},"indent":[]}},{"type":"inlineCode","value":"MapReduce","position":{"start":{"line":18,"column":29,"offset":455},"end":{"line":18,"column":40,"offset":466},"indent":[]}},{"type":"text","value":" and the ","position":{"start":{"line":18,"column":40,"offset":466},"end":{"line":18,"column":49,"offset":475},"indent":[]}},{"type":"inlineCode","value":"Google File System","position":{"start":{"line":18,"column":49,"offset":475},"end":{"line":18,"column":69,"offset":495},"indent":[]}},{"type":"text","value":" papers.","position":{"start":{"line":18,"column":69,"offset":495},"end":{"line":18,"column":77,"offset":503},"indent":[]}}],"position":{"start":{"line":18,"column":1,"offset":427},"end":{"line":18,"column":77,"offset":503},"indent":[]}},{"type":"paragraph","children":[{"type":"text","value":"It was written in Java to implement the ","position":{"start":{"line":20,"column":1,"offset":505},"end":{"line":20,"column":41,"offset":545},"indent":[]}},{"type":"inlineCode","value":"MapReduce","position":{"start":{"line":20,"column":41,"offset":545},"end":{"line":20,"column":52,"offset":556},"indent":[]}},{"type":"text","value":" programming model for scalable, reliable and distributed computing.","position":{"start":{"line":20,"column":52,"offset":556},"end":{"line":20,"column":120,"offset":624},"indent":[]}}],"position":{"start":{"line":20,"column":1,"offset":505},"end":{"line":20,"column":120,"offset":624},"indent":[]}},{"type":"paragraph","children":[{"type":"text","value":"The framework is composed of:","position":{"start":{"line":22,"column":1,"offset":626},"end":{"line":22,"column":30,"offset":655},"indent":[]}}],"position":{"start":{"line":22,"column":1,"offset":626},"end":{"line":22,"column":30,"offset":655},"indent":[]}},{"type":"list","ordered":true,"start":1,"spread":false,"children":[{"type":"listItem","spread":false,"checked":null,"children":[{"type":"paragraph","children":[{"type":"text","value":"Hadoop Common: Contains libraries and utilities needed by other Hadoop modules.","position":{"start":{"line":24,"column":4,"offset":660},"end":{"line":24,"column":83,"offset":739},"indent":[]}}],"position":{"start":{"line":24,"column":4,"offset":660},"end":{"line":24,"column":83,"offset":739},"indent":[]}}],"position":{"start":{"line":24,"column":1,"offset":657},"end":{"line":24,"column":83,"offset":739},"indent":[]}},{"type":"listItem","spread":false,"checked":null,"children":[{"type":"paragraph","children":[{"type":"text","value":"Hadoop Distributed File System (HDFS): A distributed file system that stores data on the commodity machines, providing very high aggregate bandwidth across the cluster.","position":{"start":{"line":25,"column":4,"offset":743},"end":{"line":25,"column":172,"offset":911},"indent":[]}}],"position":{"start":{"line":25,"column":4,"offset":743},"end":{"line":25,"column":172,"offset":911},"indent":[]}}],"position":{"start":{"line":25,"column":1,"offset":740},"end":{"line":25,"column":172,"offset":911},"indent":[]}},{"type":"listItem","spread":false,"checked":null,"children":[{"type":"paragraph","children":[{"type":"text","value":"Hadoop MapReduce: A programming model for large-scale data processing.","position":{"start":{"line":26,"column":4,"offset":915},"end":{"line":26,"column":74,"offset":985},"indent":[]}}],"position":{"start":{"line":26,"column":4,"offset":915},"end":{"line":26,"column":74,"offset":985},"indent":[]}}],"position":{"start":{"line":26,"column":1,"offset":912},"end":{"line":26,"column":74,"offset":985},"indent":[]}},{"type":"listItem","spread":false,"checked":null,"children":[{"type":"paragraph","children":[{"type":"text","value":"Hadoop YARN: A resource management platform responsible for managing compute resources in clusters and using them for the scheduling of users' applications.","position":{"start":{"line":27,"column":4,"offset":989},"end":{"line":27,"column":160,"offset":1145},"indent":[]}}],"position":{"start":{"line":27,"column":4,"offset":989},"end":{"line":27,"column":160,"offset":1145},"indent":[]}}],"position":{"start":{"line":27,"column":1,"offset":986},"end":{"line":27,"column":160,"offset":1145},"indent":[]}}],"position":{"start":{"line":24,"column":1,"offset":657},"end":{"line":27,"column":160,"offset":1145},"indent":[1,1,1]}},{"type":"heading","depth":2,"children":[{"type":"text","value":"HDFS","position":{"start":{"line":29,"column":4,"offset":1150},"end":{"line":29,"column":8,"offset":1154},"indent":[]}}],"position":{"start":{"line":29,"column":1,"offset":1147},"end":{"line":29,"column":8,"offset":1154},"indent":[]}},{"type":"list","ordered":false,"start":null,"spread":false,"children":[{"type":"listItem","spread":false,"checked":null,"children":[{"type":"paragraph","children":[{"type":"text","value":"Structured like a regular Unix file system with data storage distributed across several machines in the cluster.","position":{"start":{"line":31,"column":3,"offset":1158},"end":{"line":31,"column":115,"offset":1270},"indent":[]}}],"position":{"start":{"line":31,"column":3,"offset":1158},"end":{"line":31,"column":115,"offset":1270},"indent":[]}}],"position":{"start":{"line":31,"column":1,"offset":1156},"end":{"line":31,"column":115,"offset":1270},"indent":[]}},{"type":"listItem","spread":false,"checked":null,"children":[{"type":"paragraph","children":[{"type":"text","value":"Data service that sits atop regular file systems, allowing a fault tolerant, resilient clustered approach to storing and processing data.","position":{"start":{"line":32,"column":3,"offset":1273},"end":{"line":32,"column":140,"offset":1410},"indent":[]}}],"position":{"start":{"line":32,"column":3,"offset":1273},"end":{"line":32,"column":140,"offset":1410},"indent":[]}}],"position":{"start":{"line":32,"column":1,"offset":1271},"end":{"line":32,"column":140,"offset":1410},"indent":[]}},{"type":"listItem","spread":false,"checked":null,"children":[{"type":"paragraph","children":[{"type":"text","value":"Fault-tolerant: Detection of faults and quick automatic recovery is a core architectural goal.","position":{"start":{"line":33,"column":3,"offset":1413},"end":{"line":33,"column":97,"offset":1507},"indent":[]}}],"position":{"start":{"line":33,"column":3,"offset":1413},"end":{"line":33,"column":97,"offset":1507},"indent":[]}}],"position":{"start":{"line":33,"column":1,"offset":1411},"end":{"line":33,"column":97,"offset":1507},"indent":[]}},{"type":"listItem","spread":false,"checked":null,"children":[{"type":"paragraph","children":[{"type":"text","value":"Tuned to support large files. Typically a file is GB or TB and can support tens of millions of files by scaling to hundreds of nodes in a cluster.","position":{"start":{"line":34,"column":3,"offset":1510},"end":{"line":34,"column":149,"offset":1656},"indent":[]}}],"position":{"start":{"line":34,"column":3,"offset":1510},"end":{"line":34,"column":149,"offset":1656},"indent":[]}}],"position":{"start":{"line":34,"column":1,"offset":1508},"end":{"line":34,"column":149,"offset":1656},"indent":[]}},{"type":"listItem","spread":false,"checked":null,"children":[{"type":"paragraph","children":[{"type":"text","value":"Follows ","position":{"start":{"line":35,"column":3,"offset":1659},"end":{"line":35,"column":11,"offset":1667},"indent":[]}},{"type":"inlineCode","value":"write once, read multiple","position":{"start":{"line":35,"column":11,"offset":1667},"end":{"line":35,"column":38,"offset":1694},"indent":[]}},{"type":"text","value":" approach, simplifying data coherency issues and enabling high throughput data access. Example is a web crawler application.","position":{"start":{"line":35,"column":38,"offset":1694},"end":{"line":35,"column":162,"offset":1818},"indent":[]}}],"position":{"start":{"line":35,"column":3,"offset":1659},"end":{"line":35,"column":162,"offset":1818},"indent":[]}}],"position":{"start":{"line":35,"column":1,"offset":1657},"end":{"line":35,"column":162,"offset":1818},"indent":[]}},{"type":"listItem","spread":false,"checked":null,"children":[{"type":"paragraph","children":[{"type":"text","value":"Optimized for throughput rather than latency. This makes it suited for long batch operations on large scale data rather than interactive analysis on streaming data.","position":{"start":{"line":36,"column":3,"offset":1821},"end":{"line":36,"column":167,"offset":1985},"indent":[]}}],"position":{"start":{"line":36,"column":3,"offset":1821},"end":{"line":36,"column":167,"offset":1985},"indent":[]}}],"position":{"start":{"line":36,"column":1,"offset":1819},"end":{"line":36,"column":167,"offset":1985},"indent":[]}},{"type":"listItem","spread":false,"checked":null,"children":[{"type":"paragraph","children":[{"type":"text","value":"Moving computation near the data reduces network congestion and increses throughput. HDFS provides interfaces or applications to move closer to data storage.","position":{"start":{"line":37,"column":3,"offset":1988},"end":{"line":37,"column":160,"offset":2145},"indent":[]}}],"position":{"start":{"line":37,"column":3,"offset":1988},"end":{"line":37,"column":160,"offset":2145},"indent":[]}}],"position":{"start":{"line":37,"column":1,"offset":1986},"end":{"line":37,"column":160,"offset":2145},"indent":[]}}],"position":{"start":{"line":31,"column":1,"offset":1156},"end":{"line":37,"column":160,"offset":2145},"indent":[1,1,1,1,1,1]}},{"type":"heading","depth":3,"children":[{"type":"text","value":"Architecture","position":{"start":{"line":39,"column":5,"offset":2151},"end":{"line":39,"column":17,"offset":2163},"indent":[]}}],"position":{"start":{"line":39,"column":1,"offset":2147},"end":{"line":39,"column":17,"offset":2163},"indent":[]}},{"type":"list","ordered":true,"start":1,"spread":false,"children":[{"type":"listItem","spread":false,"checked":null,"children":[{"type":"paragraph","children":[{"type":"text","value":"Leader-follower architecture, where ","position":{"start":{"line":41,"column":4,"offset":2168},"end":{"line":41,"column":40,"offset":2204},"indent":[]}},{"type":"inlineCode","value":"Namenode","position":{"start":{"line":41,"column":40,"offset":2204},"end":{"line":41,"column":50,"offset":2214},"indent":[]}},{"type":"text","value":" is the leader and ","position":{"start":{"line":41,"column":50,"offset":2214},"end":{"line":41,"column":69,"offset":2233},"indent":[]}},{"type":"inlineCode","value":"Datanodes","position":{"start":{"line":41,"column":69,"offset":2233},"end":{"line":41,"column":80,"offset":2244},"indent":[]}},{"type":"text","value":" are slaves.","position":{"start":{"line":41,"column":80,"offset":2244},"end":{"line":41,"column":92,"offset":2256},"indent":[]}}],"position":{"start":{"line":41,"column":4,"offset":2168},"end":{"line":41,"column":92,"offset":2256},"indent":[]}}],"position":{"start":{"line":41,"column":1,"offset":2165},"end":{"line":41,"column":92,"offset":2256},"indent":[]}},{"type":"listItem","spread":false,"checked":null,"children":[{"type":"paragraph","children":[{"type":"text","value":"Files split into blocks, and blocks are stored on datanodes (generally one per node within cluster).","position":{"start":{"line":42,"column":4,"offset":2260},"end":{"line":42,"column":104,"offset":2360},"indent":[]}}],"position":{"start":{"line":42,"column":4,"offset":2260},"end":{"line":42,"column":104,"offset":2360},"indent":[]}}],"position":{"start":{"line":42,"column":1,"offset":2257},"end":{"line":42,"column":104,"offset":2360},"indent":[]}},{"type":"listItem","spread":false,"checked":null,"children":[{"type":"paragraph","children":[{"type":"text","value":"Datanodes manage storage attached to nodes that they run on.","position":{"start":{"line":43,"column":4,"offset":2364},"end":{"line":43,"column":64,"offset":2424},"indent":[]}}],"position":{"start":{"line":43,"column":4,"offset":2364},"end":{"line":43,"column":64,"offset":2424},"indent":[]}}],"position":{"start":{"line":43,"column":1,"offset":2361},"end":{"line":43,"column":64,"offset":2424},"indent":[]}},{"type":"listItem","spread":false,"checked":null,"children":[{"type":"paragraph","children":[{"type":"text","value":"Namenode controls all metadata, including what blocks make up a file and which datanode the blocks are stored on.","position":{"start":{"line":44,"column":4,"offset":2428},"end":{"line":44,"column":117,"offset":2541},"indent":[]}}],"position":{"start":{"line":44,"column":4,"offset":2428},"end":{"line":44,"column":117,"offset":2541},"indent":[]}}],"position":{"start":{"line":44,"column":1,"offset":2425},"end":{"line":44,"column":117,"offset":2541},"indent":[]}},{"type":"listItem","spread":false,"checked":null,"children":[{"type":"paragraph","children":[{"type":"text","value":"Namenode executres file system operations like opening, closing and renaming files and directories.","position":{"start":{"line":45,"column":4,"offset":2545},"end":{"line":45,"column":103,"offset":2644},"indent":[]}}],"position":{"start":{"line":45,"column":4,"offset":2545},"end":{"line":45,"column":103,"offset":2644},"indent":[]}}],"position":{"start":{"line":45,"column":1,"offset":2542},"end":{"line":45,"column":103,"offset":2644},"indent":[]}},{"type":"listItem","spread":false,"checked":null,"children":[{"type":"paragraph","children":[{"type":"text","value":"Datanodes serve read and write requests from the clients.","position":{"start":{"line":46,"column":4,"offset":2648},"end":{"line":46,"column":61,"offset":2705},"indent":[]}}],"position":{"start":{"line":46,"column":4,"offset":2648},"end":{"line":46,"column":61,"offset":2705},"indent":[]}}],"position":{"start":{"line":46,"column":1,"offset":2645},"end":{"line":46,"column":61,"offset":2705},"indent":[]}},{"type":"listItem","spread":false,"checked":null,"children":[{"type":"paragraph","children":[{"type":"text","value":"Datanodes perform block creation, deletion, replication upon instruction from the Namenode.","position":{"start":{"line":47,"column":4,"offset":2709},"end":{"line":47,"column":95,"offset":2800},"indent":[]}}],"position":{"start":{"line":47,"column":4,"offset":2709},"end":{"line":47,"column":95,"offset":2800},"indent":[]}}],"position":{"start":{"line":47,"column":1,"offset":2706},"end":{"line":47,"column":95,"offset":2800},"indent":[]}},{"type":"listItem","spread":false,"checked":null,"children":[{"type":"paragraph","children":[{"type":"text","value":"Namenode and Datanode are Java software designed to run on commodity hardware that supports Java.","position":{"start":{"line":48,"column":4,"offset":2804},"end":{"line":48,"column":101,"offset":2901},"indent":[]}}],"position":{"start":{"line":48,"column":4,"offset":2804},"end":{"line":48,"column":101,"offset":2901},"indent":[]}}],"position":{"start":{"line":48,"column":1,"offset":2801},"end":{"line":48,"column":101,"offset":2901},"indent":[]}},{"type":"listItem","spread":false,"checked":null,"children":[{"type":"paragraph","children":[{"type":"text","value":"Usually a cluster contains a single Namenode and multiple datanodes, one each for each node in the cluster.","position":{"start":{"line":49,"column":4,"offset":2905},"end":{"line":49,"column":111,"offset":3012},"indent":[]}}],"position":{"start":{"line":49,"column":4,"offset":2905},"end":{"line":49,"column":111,"offset":3012},"indent":[]}}],"position":{"start":{"line":49,"column":1,"offset":2902},"end":{"line":49,"column":111,"offset":3012},"indent":[]}}],"position":{"start":{"line":41,"column":1,"offset":2165},"end":{"line":49,"column":111,"offset":3012},"indent":[1,1,1,1,1,1,1,1]}},{"type":"paragraph","children":[{"type":"image","title":null,"url":"https://hadoop.apache.org/docs/r1.2.1/images/hdfsarchitecture.gif","alt":"HDFS Architecture","position":{"start":{"line":51,"column":1,"offset":3014},"end":{"line":51,"column":88,"offset":3101},"indent":[]}}],"position":{"start":{"line":51,"column":1,"offset":3014},"end":{"line":51,"column":88,"offset":3101},"indent":[]}},{"type":"paragraph","children":[{"type":"text","value":"The ","position":{"start":{"line":53,"column":1,"offset":3103},"end":{"line":53,"column":5,"offset":3107},"indent":[]}},{"type":"inlineCode","value":"Namenode","position":{"start":{"line":53,"column":5,"offset":3107},"end":{"line":53,"column":15,"offset":3117},"indent":[]}},{"type":"text","value":" makes all decisions around replication of blocks for data durability. Periodically receives heartbeat and ","position":{"start":{"line":53,"column":15,"offset":3117},"end":{"line":53,"column":122,"offset":3224},"indent":[]}},{"type":"inlineCode","value":"BlockReport","position":{"start":{"line":53,"column":122,"offset":3224},"end":{"line":53,"column":135,"offset":3237},"indent":[]}},{"type":"text","value":" from datanodes in the cluster. Receipt of heartbeat is the health check.","position":{"start":{"line":53,"column":135,"offset":3237},"end":{"line":53,"column":208,"offset":3310},"indent":[]}}],"position":{"start":{"line":53,"column":1,"offset":3103},"end":{"line":53,"column":208,"offset":3310},"indent":[]}},{"type":"heading","depth":2,"children":[{"type":"text","value":"Hadoop MapReduce","position":{"start":{"line":55,"column":4,"offset":3315},"end":{"line":55,"column":20,"offset":3331},"indent":[]}}],"position":{"start":{"line":55,"column":1,"offset":3312},"end":{"line":55,"column":20,"offset":3331},"indent":[]}},{"type":"paragraph","children":[{"type":"text","value":"A framework that makes it easy to write applications which can consume huge amouts of data.","position":{"start":{"line":57,"column":1,"offset":3333},"end":{"line":57,"column":92,"offset":3424},"indent":[]}}],"position":{"start":{"line":57,"column":1,"offset":3333},"end":{"line":57,"column":92,"offset":3424},"indent":[]}},{"type":"paragraph","children":[{"type":"text","value":"It allows processing in parallel on large clusters consisting of thousands of nodes in a manner that is reliable and fault tolerant.","position":{"start":{"line":59,"column":1,"offset":3426},"end":{"line":59,"column":133,"offset":3558},"indent":[]}}],"position":{"start":{"line":59,"column":1,"offset":3426},"end":{"line":59,"column":133,"offset":3558},"indent":[]}},{"type":"paragraph","children":[{"type":"text","value":"The MapReduce layer consists of:","position":{"start":{"line":61,"column":1,"offset":3560},"end":{"line":61,"column":33,"offset":3592},"indent":[]}}],"position":{"start":{"line":61,"column":1,"offset":3560},"end":{"line":61,"column":33,"offset":3592},"indent":[]}},{"type":"list","ordered":true,"start":1,"spread":false,"children":[{"type":"listItem","spread":false,"checked":null,"children":[{"type":"paragraph","children":[{"type":"text","value":"MapReduce Java API to write workflows","position":{"start":{"line":63,"column":4,"offset":3597},"end":{"line":63,"column":41,"offset":3634},"indent":[]}}],"position":{"start":{"line":63,"column":4,"offset":3597},"end":{"line":63,"column":41,"offset":3634},"indent":[]}}],"position":{"start":{"line":63,"column":1,"offset":3594},"end":{"line":63,"column":41,"offset":3634},"indent":[]}},{"type":"listItem","spread":false,"checked":null,"children":[{"type":"paragraph","children":[{"type":"text","value":"Services to manage these workflows and provide the scheduling, distribution and parallelizing.","position":{"start":{"line":64,"column":4,"offset":3638},"end":{"line":64,"column":98,"offset":3732},"indent":[]}}],"position":{"start":{"line":64,"column":4,"offset":3638},"end":{"line":64,"column":98,"offset":3732},"indent":[]}}],"position":{"start":{"line":64,"column":1,"offset":3635},"end":{"line":64,"column":98,"offset":3732},"indent":[]}}],"position":{"start":{"line":63,"column":1,"offset":3594},"end":{"line":64,"column":98,"offset":3732},"indent":[1]}},{"type":"heading","depth":3,"children":[{"type":"text","value":"MapReduce jobs","position":{"start":{"line":66,"column":5,"offset":3738},"end":{"line":66,"column":19,"offset":3752},"indent":[]}}],"position":{"start":{"line":66,"column":1,"offset":3734},"end":{"line":66,"column":19,"offset":3752},"indent":[]}},{"type":"list","ordered":true,"start":1,"spread":false,"children":[{"type":"listItem","spread":false,"checked":null,"children":[{"type":"paragraph","children":[{"type":"text","value":"Splits the data sets into independent chunks.","position":{"start":{"line":68,"column":4,"offset":3757},"end":{"line":68,"column":49,"offset":3802},"indent":[]}}],"position":{"start":{"line":68,"column":4,"offset":3757},"end":{"line":68,"column":49,"offset":3802},"indent":[]}}],"position":{"start":{"line":68,"column":1,"offset":3754},"end":{"line":68,"column":49,"offset":3802},"indent":[]}},{"type":"listItem","spread":false,"checked":null,"children":[{"type":"paragraph","children":[{"type":"text","value":"Data sets are processed by map tasks in parallel.","position":{"start":{"line":69,"column":4,"offset":3806},"end":{"line":69,"column":53,"offset":3855},"indent":[]}}],"position":{"start":{"line":69,"column":4,"offset":3806},"end":{"line":69,"column":53,"offset":3855},"indent":[]}}],"position":{"start":{"line":69,"column":1,"offset":3803},"end":{"line":69,"column":53,"offset":3855},"indent":[]}},{"type":"listItem","spread":false,"checked":null,"children":[{"type":"paragraph","children":[{"type":"text","value":"MapReduce sorts the output of map jobs and feeds them to reduce tasks.","position":{"start":{"line":70,"column":4,"offset":3859},"end":{"line":70,"column":74,"offset":3929},"indent":[]}}],"position":{"start":{"line":70,"column":4,"offset":3859},"end":{"line":70,"column":74,"offset":3929},"indent":[]}}],"position":{"start":{"line":70,"column":1,"offset":3856},"end":{"line":70,"column":74,"offset":3929},"indent":[]}},{"type":"listItem","spread":false,"checked":null,"children":[{"type":"paragraph","children":[{"type":"text","value":"Both input and output of map and reduce are stored on the file system.","position":{"start":{"line":71,"column":4,"offset":3933},"end":{"line":71,"column":74,"offset":4003},"indent":[]}}],"position":{"start":{"line":71,"column":4,"offset":3933},"end":{"line":71,"column":74,"offset":4003},"indent":[]}}],"position":{"start":{"line":71,"column":1,"offset":3930},"end":{"line":71,"column":74,"offset":4003},"indent":[]}},{"type":"listItem","spread":false,"checked":null,"children":[{"type":"paragraph","children":[{"type":"text","value":"Framework takes care of scheduling tasks, monitoring them and re-executing failed tasks.","position":{"start":{"line":72,"column":4,"offset":4007},"end":{"line":72,"column":92,"offset":4095},"indent":[]}}],"position":{"start":{"line":72,"column":4,"offset":4007},"end":{"line":72,"column":92,"offset":4095},"indent":[]}}],"position":{"start":{"line":72,"column":1,"offset":4004},"end":{"line":72,"column":92,"offset":4095},"indent":[]}},{"type":"listItem","spread":false,"checked":null,"children":[{"type":"paragraph","children":[{"type":"text","value":"MapReduce framework and HDFS are running on the same set of nodes. Tasks are scheduled on nodes where data is already present, hence yielding high bandwidth across the cluster.","position":{"start":{"line":73,"column":4,"offset":4099},"end":{"line":73,"column":180,"offset":4275},"indent":[]}}],"position":{"start":{"line":73,"column":4,"offset":4099},"end":{"line":73,"column":180,"offset":4275},"indent":[]}}],"position":{"start":{"line":73,"column":1,"offset":4096},"end":{"line":73,"column":180,"offset":4275},"indent":[]}}],"position":{"start":{"line":68,"column":1,"offset":3754},"end":{"line":73,"column":180,"offset":4275},"indent":[1,1,1,1,1]}},{"type":"heading","depth":3,"children":[{"type":"text","value":"Inputs and Outputs of a MapReduce Job","position":{"start":{"line":75,"column":5,"offset":4281},"end":{"line":75,"column":42,"offset":4318},"indent":[]}}],"position":{"start":{"line":75,"column":1,"offset":4277},"end":{"line":75,"column":42,"offset":4318},"indent":[]}},{"type":"list","ordered":false,"start":null,"spread":false,"children":[{"type":"listItem","spread":false,"checked":null,"children":[{"type":"paragraph","children":[{"type":"text","value":"Exclusively operates on key-value pairs.","position":{"start":{"line":77,"column":3,"offset":4322},"end":{"line":77,"column":43,"offset":4362},"indent":[]}}],"position":{"start":{"line":77,"column":3,"offset":4322},"end":{"line":77,"column":43,"offset":4362},"indent":[]}}],"position":{"start":{"line":77,"column":1,"offset":4320},"end":{"line":77,"column":43,"offset":4362},"indent":[]}},{"type":"listItem","spread":false,"checked":null,"children":[{"type":"paragraph","children":[{"type":"text","value":"Input is large scale data set which benefits from parallel processing and does not fit on a single machine.","position":{"start":{"line":78,"column":3,"offset":4365},"end":{"line":78,"column":110,"offset":4472},"indent":[]}}],"position":{"start":{"line":78,"column":3,"offset":4365},"end":{"line":78,"column":110,"offset":4472},"indent":[]}}],"position":{"start":{"line":78,"column":1,"offset":4363},"end":{"line":78,"column":110,"offset":4472},"indent":[]}},{"type":"listItem","spread":false,"checked":null,"children":[{"type":"paragraph","children":[{"type":"text","value":"Input split into independent data sets and map function produces key-value pair for each record in the data set.","position":{"start":{"line":79,"column":3,"offset":4475},"end":{"line":79,"column":115,"offset":4587},"indent":[]}}],"position":{"start":{"line":79,"column":3,"offset":4475},"end":{"line":79,"column":115,"offset":4587},"indent":[]}}],"position":{"start":{"line":79,"column":1,"offset":4473},"end":{"line":79,"column":115,"offset":4587},"indent":[]}},{"type":"listItem","spread":false,"checked":null,"children":[{"type":"paragraph","children":[{"type":"text","value":"Output of mappers is shuffled, sorted, grouped and passed to the reducers.","position":{"start":{"line":80,"column":3,"offset":4590},"end":{"line":80,"column":77,"offset":4664},"indent":[]}}],"position":{"start":{"line":80,"column":3,"offset":4590},"end":{"line":80,"column":77,"offset":4664},"indent":[]}}],"position":{"start":{"line":80,"column":1,"offset":4588},"end":{"line":80,"column":77,"offset":4664},"indent":[]}},{"type":"listItem","spread":false,"checked":null,"children":[{"type":"paragraph","children":[{"type":"text","value":"Reducer function applied to sets of key-value pairs that share the same key. The reducer function often agregates the value for the pairs with the same key.","position":{"start":{"line":81,"column":3,"offset":4667},"end":{"line":81,"column":159,"offset":4823},"indent":[]}}],"position":{"start":{"line":81,"column":3,"offset":4667},"end":{"line":81,"column":159,"offset":4823},"indent":[]}}],"position":{"start":{"line":81,"column":1,"offset":4665},"end":{"line":81,"column":159,"offset":4823},"indent":[]}}],"position":{"start":{"line":77,"column":1,"offset":4320},"end":{"line":81,"column":159,"offset":4823},"indent":[1,1,1,1]}},{"type":"paragraph","children":[{"type":"text","value":"It is important to know that:","position":{"start":{"line":83,"column":1,"offset":4825},"end":{"line":83,"column":30,"offset":4854},"indent":[]}}],"position":{"start":{"line":83,"column":1,"offset":4825},"end":{"line":83,"column":30,"offset":4854},"indent":[]}},{"type":"list","ordered":true,"start":1,"spread":false,"children":[{"type":"listItem","spread":false,"checked":null,"children":[{"type":"paragraph","children":[{"type":"text","value":"Almost all data can be mapped to a key-value pair using a map function.","position":{"start":{"line":85,"column":4,"offset":4859},"end":{"line":85,"column":75,"offset":4930},"indent":[]}}],"position":{"start":{"line":85,"column":4,"offset":4859},"end":{"line":85,"column":75,"offset":4930},"indent":[]}}],"position":{"start":{"line":85,"column":1,"offset":4856},"end":{"line":85,"column":75,"offset":4930},"indent":[]}},{"type":"listItem","spread":false,"checked":null,"children":[{"type":"paragraph","children":[{"type":"text","value":"Keys and values can be of any type. If using a custom type, the type must be implement a writable interface.","position":{"start":{"line":86,"column":4,"offset":4934},"end":{"line":86,"column":112,"offset":5042},"indent":[]}}],"position":{"start":{"line":86,"column":4,"offset":4934},"end":{"line":86,"column":112,"offset":5042},"indent":[]}}],"position":{"start":{"line":86,"column":1,"offset":4931},"end":{"line":86,"column":112,"offset":5042},"indent":[]}},{"type":"listItem","spread":false,"checked":null,"children":[{"type":"paragraph","children":[{"type":"inlineCode","value":"MapReduce","position":{"start":{"line":87,"column":4,"offset":5046},"end":{"line":87,"column":15,"offset":5057},"indent":[]}},{"type":"text","value":" cannot be used if a computation of a value depends on a previously computed value. Recursive funcs like Fibonnaci cannot be implemented using ","position":{"start":{"line":87,"column":15,"offset":5057},"end":{"line":87,"column":158,"offset":5200},"indent":[]}},{"type":"inlineCode","value":"MapReduce","position":{"start":{"line":87,"column":158,"offset":5200},"end":{"line":87,"column":169,"offset":5211},"indent":[]}},{"type":"text","value":".","position":{"start":{"line":87,"column":169,"offset":5211},"end":{"line":87,"column":170,"offset":5212},"indent":[]}}],"position":{"start":{"line":87,"column":4,"offset":5046},"end":{"line":87,"column":170,"offset":5212},"indent":[]}}],"position":{"start":{"line":87,"column":1,"offset":5043},"end":{"line":87,"column":170,"offset":5212},"indent":[]}}],"position":{"start":{"line":85,"column":1,"offset":4856},"end":{"line":87,"column":170,"offset":5212},"indent":[1,1]}},{"type":"paragraph","children":[{"type":"text","value":"This is an example of a word count ","position":{"start":{"line":89,"column":1,"offset":5214},"end":{"line":89,"column":36,"offset":5249},"indent":[]}},{"type":"inlineCode","value":"MapReduce","position":{"start":{"line":89,"column":36,"offset":5249},"end":{"line":89,"column":47,"offset":5260},"indent":[]}},{"type":"text","value":" job.","position":{"start":{"line":89,"column":47,"offset":5260},"end":{"line":89,"column":52,"offset":5265},"indent":[]}}],"position":{"start":{"line":89,"column":1,"offset":5214},"end":{"line":89,"column":52,"offset":5265},"indent":[]}},{"type":"paragraph","children":[{"type":"image","title":null,"url":"https://www.guru99.com/images/Big_Data/061114_0930_Introductio1.png","alt":"Example of MapReduce job","position":{"start":{"line":91,"column":1,"offset":5267},"end":{"line":91,"column":97,"offset":5363},"indent":[]}}],"position":{"start":{"line":91,"column":1,"offset":5267},"end":{"line":91,"column":97,"offset":5363},"indent":[]}},{"type":"paragraph","children":[{"type":"text","value":"The order of a job goes as the following:","position":{"start":{"line":93,"column":1,"offset":5365},"end":{"line":93,"column":42,"offset":5406},"indent":[]}}],"position":{"start":{"line":93,"column":1,"offset":5365},"end":{"line":93,"column":42,"offset":5406},"indent":[]}},{"type":"list","ordered":true,"start":1,"spread":false,"children":[{"type":"listItem","spread":false,"checked":null,"children":[{"type":"paragraph","children":[{"type":"text","value":"Input","position":{"start":{"line":95,"column":4,"offset":5411},"end":{"line":95,"column":9,"offset":5416},"indent":[]}}],"position":{"start":{"line":95,"column":4,"offset":5411},"end":{"line":95,"column":9,"offset":5416},"indent":[]}}],"position":{"start":{"line":95,"column":1,"offset":5408},"end":{"line":95,"column":9,"offset":5416},"indent":[]}},{"type":"listItem","spread":false,"checked":null,"children":[{"type":"paragraph","children":[{"type":"text","value":"Splitting","position":{"start":{"line":96,"column":4,"offset":5420},"end":{"line":96,"column":13,"offset":5429},"indent":[]}}],"position":{"start":{"line":96,"column":4,"offset":5420},"end":{"line":96,"column":13,"offset":5429},"indent":[]}}],"position":{"start":{"line":96,"column":1,"offset":5417},"end":{"line":96,"column":13,"offset":5429},"indent":[]}},{"type":"listItem","spread":false,"checked":null,"children":[{"type":"paragraph","children":[{"type":"text","value":"Mapping","position":{"start":{"line":97,"column":4,"offset":5433},"end":{"line":97,"column":11,"offset":5440},"indent":[]}}],"position":{"start":{"line":97,"column":4,"offset":5433},"end":{"line":97,"column":11,"offset":5440},"indent":[]}}],"position":{"start":{"line":97,"column":1,"offset":5430},"end":{"line":97,"column":11,"offset":5440},"indent":[]}},{"type":"listItem","spread":false,"checked":null,"children":[{"type":"paragraph","children":[{"type":"text","value":"Shuffle","position":{"start":{"line":98,"column":4,"offset":5444},"end":{"line":98,"column":11,"offset":5451},"indent":[]}}],"position":{"start":{"line":98,"column":4,"offset":5444},"end":{"line":98,"column":11,"offset":5451},"indent":[]}}],"position":{"start":{"line":98,"column":1,"offset":5441},"end":{"line":98,"column":11,"offset":5451},"indent":[]}},{"type":"listItem","spread":false,"checked":null,"children":[{"type":"paragraph","children":[{"type":"text","value":"Reduce","position":{"start":{"line":99,"column":4,"offset":5455},"end":{"line":99,"column":10,"offset":5461},"indent":[]}}],"position":{"start":{"line":99,"column":4,"offset":5455},"end":{"line":99,"column":10,"offset":5461},"indent":[]}}],"position":{"start":{"line":99,"column":1,"offset":5452},"end":{"line":99,"column":10,"offset":5461},"indent":[]}},{"type":"listItem","spread":false,"checked":null,"children":[{"type":"paragraph","children":[{"type":"text","value":"Final Result","position":{"start":{"line":100,"column":4,"offset":5465},"end":{"line":100,"column":16,"offset":5477},"indent":[]}}],"position":{"start":{"line":100,"column":4,"offset":5465},"end":{"line":100,"column":16,"offset":5477},"indent":[]}}],"position":{"start":{"line":100,"column":1,"offset":5462},"end":{"line":100,"column":16,"offset":5477},"indent":[]}}],"position":{"start":{"line":95,"column":1,"offset":5408},"end":{"line":100,"column":16,"offset":5477},"indent":[1,1,1,1,1]}},{"type":"heading","depth":3,"children":[{"type":"text","value":"Example Applications of MapReduce","position":{"start":{"line":102,"column":5,"offset":5483},"end":{"line":102,"column":38,"offset":5516},"indent":[]}}],"position":{"start":{"line":102,"column":1,"offset":5479},"end":{"line":102,"column":38,"offset":5516},"indent":[]}},{"type":"list","ordered":true,"start":1,"spread":false,"children":[{"type":"listItem","spread":false,"checked":null,"children":[{"type":"paragraph","children":[{"type":"text","value":"Counting votes by processing data from each polling booth.","position":{"start":{"line":104,"column":4,"offset":5521},"end":{"line":104,"column":62,"offset":5579},"indent":[]}}],"position":{"start":{"line":104,"column":4,"offset":5521},"end":{"line":104,"column":62,"offset":5579},"indent":[]}}],"position":{"start":{"line":104,"column":1,"offset":5518},"end":{"line":104,"column":62,"offset":5579},"indent":[]}},{"type":"listItem","spread":false,"checked":null,"children":[{"type":"paragraph","children":[{"type":"text","value":"Aggregating electricy consumption from data points collected across a large geographical area.","position":{"start":{"line":105,"column":4,"offset":5583},"end":{"line":105,"column":98,"offset":5677},"indent":[]}}],"position":{"start":{"line":105,"column":4,"offset":5583},"end":{"line":105,"column":98,"offset":5677},"indent":[]}}],"position":{"start":{"line":105,"column":1,"offset":5580},"end":{"line":105,"column":98,"offset":5677},"indent":[]}},{"type":"listItem","spread":false,"checked":null,"children":[{"type":"paragraph","children":[{"type":"text","value":"Used by Google Maps to calculate nearest neighbour.","position":{"start":{"line":106,"column":4,"offset":5681},"end":{"line":106,"column":55,"offset":5732},"indent":[]}}],"position":{"start":{"line":106,"column":4,"offset":5681},"end":{"line":106,"column":55,"offset":5732},"indent":[]}}],"position":{"start":{"line":106,"column":1,"offset":5678},"end":{"line":106,"column":55,"offset":5732},"indent":[]}},{"type":"listItem","spread":false,"checked":null,"children":[{"type":"paragraph","children":[{"type":"text","value":"Performing statistical aggregate type functions on large data sets.","position":{"start":{"line":107,"column":4,"offset":5736},"end":{"line":107,"column":71,"offset":5803},"indent":[]}}],"position":{"start":{"line":107,"column":4,"offset":5736},"end":{"line":107,"column":71,"offset":5803},"indent":[]}}],"position":{"start":{"line":107,"column":1,"offset":5733},"end":{"line":107,"column":71,"offset":5803},"indent":[]}},{"type":"listItem","spread":false,"checked":null,"children":[{"type":"paragraph","children":[{"type":"text","value":"Counting number of href links in web log files for clickstream analysis.","position":{"start":{"line":108,"column":4,"offset":5807},"end":{"line":108,"column":76,"offset":5879},"indent":[]}}],"position":{"start":{"line":108,"column":4,"offset":5807},"end":{"line":108,"column":76,"offset":5879},"indent":[]}}],"position":{"start":{"line":108,"column":1,"offset":5804},"end":{"line":108,"column":76,"offset":5879},"indent":[]}}],"position":{"start":{"line":104,"column":1,"offset":5518},"end":{"line":108,"column":76,"offset":5879},"indent":[1,1,1,1]}},{"type":"heading","depth":3,"children":[{"type":"text","value":"Writing and Running Hadoop MapReduce Jobs","position":{"start":{"line":110,"column":5,"offset":5885},"end":{"line":110,"column":46,"offset":5926},"indent":[]}}],"position":{"start":{"line":110,"column":1,"offset":5881},"end":{"line":110,"column":46,"offset":5926},"indent":[]}},{"type":"paragraph","children":[{"type":"text","value":"Typicall jobs are written in Java, but can also be written using:","position":{"start":{"line":112,"column":1,"offset":5928},"end":{"line":112,"column":66,"offset":5993},"indent":[]}}],"position":{"start":{"line":112,"column":1,"offset":5928},"end":{"line":112,"column":66,"offset":5993},"indent":[]}},{"type":"list","ordered":true,"start":1,"spread":false,"children":[{"type":"listItem","spread":false,"checked":null,"children":[{"type":"paragraph","children":[{"type":"text","value":"Hadoop Streaming: A utility which allows users to create an run MapReduce jobs with any executables.","position":{"start":{"line":114,"column":4,"offset":5998},"end":{"line":114,"column":104,"offset":6098},"indent":[]}}],"position":{"start":{"line":114,"column":4,"offset":5998},"end":{"line":114,"column":104,"offset":6098},"indent":[]}}],"position":{"start":{"line":114,"column":1,"offset":5995},"end":{"line":114,"column":104,"offset":6098},"indent":[]}},{"type":"listItem","spread":false,"checked":null,"children":[{"type":"paragraph","children":[{"type":"text","value":"Hadoop Pipes: C++ API to implement MapReduce applications","position":{"start":{"line":115,"column":4,"offset":6102},"end":{"line":115,"column":61,"offset":6159},"indent":[]}}],"position":{"start":{"line":115,"column":4,"offset":6102},"end":{"line":115,"column":61,"offset":6159},"indent":[]}}],"position":{"start":{"line":115,"column":1,"offset":6099},"end":{"line":115,"column":61,"offset":6159},"indent":[]}}],"position":{"start":{"line":114,"column":1,"offset":5995},"end":{"line":115,"column":61,"offset":6159},"indent":[1]}},{"type":"heading","depth":3,"children":[{"type":"text","value":"Hadoop Job configurations","position":{"start":{"line":117,"column":5,"offset":6165},"end":{"line":117,"column":30,"offset":6190},"indent":[]}}],"position":{"start":{"line":117,"column":1,"offset":6161},"end":{"line":117,"column":30,"offset":6190},"indent":[]}},{"type":"paragraph","children":[{"type":"text","value":"Consists of:","position":{"start":{"line":119,"column":1,"offset":6192},"end":{"line":119,"column":13,"offset":6204},"indent":[]}}],"position":{"start":{"line":119,"column":1,"offset":6192},"end":{"line":119,"column":13,"offset":6204},"indent":[]}},{"type":"list","ordered":false,"start":null,"spread":false,"children":[{"type":"listItem","spread":false,"checked":null,"children":[{"type":"paragraph","children":[{"type":"text","value":"Input and output locations on HDFS.","position":{"start":{"line":121,"column":3,"offset":6208},"end":{"line":121,"column":38,"offset":6243},"indent":[]}}],"position":{"start":{"line":121,"column":3,"offset":6208},"end":{"line":121,"column":38,"offset":6243},"indent":[]}}],"position":{"start":{"line":121,"column":1,"offset":6206},"end":{"line":121,"column":38,"offset":6243},"indent":[]}},{"type":"listItem","spread":false,"checked":null,"children":[{"type":"paragraph","children":[{"type":"text","value":"Map and reduce functions via implementations of interfaces or abstract classes.","position":{"start":{"line":122,"column":3,"offset":6246},"end":{"line":122,"column":82,"offset":6325},"indent":[]}}],"position":{"start":{"line":122,"column":3,"offset":6246},"end":{"line":122,"column":82,"offset":6325},"indent":[]}}],"position":{"start":{"line":122,"column":1,"offset":6244},"end":{"line":122,"column":82,"offset":6325},"indent":[]}},{"type":"listItem","spread":false,"checked":null,"children":[{"type":"paragraph","children":[{"type":"text","value":"Other job parameters.","position":{"start":{"line":123,"column":3,"offset":6328},"end":{"line":123,"column":24,"offset":6349},"indent":[]}}],"position":{"start":{"line":123,"column":3,"offset":6328},"end":{"line":123,"column":24,"offset":6349},"indent":[]}}],"position":{"start":{"line":123,"column":1,"offset":6326},"end":{"line":123,"column":24,"offset":6349},"indent":[]}}],"position":{"start":{"line":121,"column":1,"offset":6206},"end":{"line":123,"column":24,"offset":6349},"indent":[1,1]}},{"type":"paragraph","children":[{"type":"text","value":"A Hadoop job client submits the job (jar/executable) and configuration to the ","position":{"start":{"line":125,"column":1,"offset":6351},"end":{"line":125,"column":79,"offset":6429},"indent":[]}},{"type":"inlineCode","value":"ResourceManager","position":{"start":{"line":125,"column":79,"offset":6429},"end":{"line":125,"column":96,"offset":6446},"indent":[]}},{"type":"text","value":" in ","position":{"start":{"line":125,"column":96,"offset":6446},"end":{"line":125,"column":100,"offset":6450},"indent":[]}},{"type":"inlineCode","value":"YARN","position":{"start":{"line":125,"column":100,"offset":6450},"end":{"line":125,"column":106,"offset":6456},"indent":[]}},{"type":"text","value":" which distributes them to the workers and performs functions like scheduling, monitoring and providing status and diagnostic information.","position":{"start":{"line":125,"column":106,"offset":6456},"end":{"line":125,"column":244,"offset":6594},"indent":[]}}],"position":{"start":{"line":125,"column":1,"offset":6351},"end":{"line":125,"column":244,"offset":6594},"indent":[]}},{"type":"heading","depth":2,"children":[{"type":"text","value":"Yet Another Resource Negotiator (YARN)","position":{"start":{"line":127,"column":4,"offset":6599},"end":{"line":127,"column":42,"offset":6637},"indent":[]}}],"position":{"start":{"line":127,"column":1,"offset":6596},"end":{"line":127,"column":42,"offset":6637},"indent":[]}},{"type":"paragraph","children":[{"type":"text","value":"Introduced in Hadoop 2.0, YARN provides a general processing platform not constrained to ","position":{"start":{"line":129,"column":1,"offset":6639},"end":{"line":129,"column":90,"offset":6728},"indent":[]}},{"type":"inlineCode","value":"MapReduce","position":{"start":{"line":129,"column":90,"offset":6728},"end":{"line":129,"column":101,"offset":6739},"indent":[]}},{"type":"text","value":".","position":{"start":{"line":129,"column":101,"offset":6739},"end":{"line":129,"column":102,"offset":6740},"indent":[]}}],"position":{"start":{"line":129,"column":1,"offset":6639},"end":{"line":129,"column":102,"offset":6740},"indent":[]}},{"type":"paragraph","children":[{"type":"inlineCode","value":"Global ResourceManager","position":{"start":{"line":131,"column":1,"offset":6742},"end":{"line":131,"column":25,"offset":6766},"indent":[]}},{"type":"text","value":" is the authority that delegates resources among the applications in the system.","position":{"start":{"line":131,"column":25,"offset":6766},"end":{"line":131,"column":105,"offset":6846},"indent":[]}}],"position":{"start":{"line":131,"column":1,"offset":6742},"end":{"line":131,"column":105,"offset":6846},"indent":[]}},{"type":"paragraph","children":[{"type":"text","value":"It has a ","position":{"start":{"line":133,"column":1,"offset":6848},"end":{"line":133,"column":10,"offset":6857},"indent":[]}},{"type":"inlineCode","value":"NodeManager","position":{"start":{"line":133,"column":10,"offset":6857},"end":{"line":133,"column":23,"offset":6870},"indent":[]}},{"type":"text","value":" on each node that is responsible for containers, monitoring their resource usage (CPU, memory, disk, network) and reporting this data to the ","position":{"start":{"line":133,"column":23,"offset":6870},"end":{"line":133,"column":165,"offset":7012},"indent":[]}},{"type":"inlineCode","value":"ResourceManager","position":{"start":{"line":133,"column":165,"offset":7012},"end":{"line":133,"column":182,"offset":7029},"indent":[]}},{"type":"text","value":".","position":{"start":{"line":133,"column":182,"offset":7029},"end":{"line":133,"column":183,"offset":7030},"indent":[]}}],"position":{"start":{"line":133,"column":1,"offset":6848},"end":{"line":133,"column":183,"offset":7030},"indent":[]}},{"type":"paragraph","children":[{"type":"text","value":"The ","position":{"start":{"line":135,"column":1,"offset":7032},"end":{"line":135,"column":5,"offset":7036},"indent":[]}},{"type":"inlineCode","value":"ResourceManager","position":{"start":{"line":135,"column":5,"offset":7036},"end":{"line":135,"column":22,"offset":7053},"indent":[]}},{"type":"text","value":" has two components:","position":{"start":{"line":135,"column":22,"offset":7053},"end":{"line":135,"column":42,"offset":7073},"indent":[]}}],"position":{"start":{"line":135,"column":1,"offset":7032},"end":{"line":135,"column":42,"offset":7073},"indent":[]}},{"type":"list","ordered":true,"start":1,"spread":false,"children":[{"type":"listItem","spread":false,"checked":null,"children":[{"type":"paragraph","children":[{"type":"text","value":"Scheduler - responsible for allocating resources the various running applications.","position":{"start":{"line":137,"column":4,"offset":7078},"end":{"line":137,"column":86,"offset":7160},"indent":[]}}],"position":{"start":{"line":137,"column":4,"offset":7078},"end":{"line":137,"column":86,"offset":7160},"indent":[]}}],"position":{"start":{"line":137,"column":1,"offset":7075},"end":{"line":137,"column":86,"offset":7160},"indent":[]}},{"type":"listItem","spread":false,"checked":null,"children":[{"type":"paragraph","children":[{"type":"text","value":"ApplicationsManager - responsible for accepting job-submissions, negotiating the first container for executing the application-specific ","position":{"start":{"line":138,"column":4,"offset":7164},"end":{"line":138,"column":140,"offset":7300},"indent":[]}},{"type":"inlineCode","value":"ApplicationMaster","position":{"start":{"line":138,"column":140,"offset":7300},"end":{"line":138,"column":159,"offset":7319},"indent":[]}},{"type":"text","value":" and provides the service for restarting the ","position":{"start":{"line":138,"column":159,"offset":7319},"end":{"line":138,"column":204,"offset":7364},"indent":[]}},{"type":"inlineCode","value":"ApplicationMaster","position":{"start":{"line":138,"column":204,"offset":7364},"end":{"line":138,"column":223,"offset":7383},"indent":[]}},{"type":"text","value":" on failure.","position":{"start":{"line":138,"column":223,"offset":7383},"end":{"line":138,"column":235,"offset":7395},"indent":[]}}],"position":{"start":{"line":138,"column":4,"offset":7164},"end":{"line":138,"column":235,"offset":7395},"indent":[]}}],"position":{"start":{"line":138,"column":1,"offset":7161},"end":{"line":138,"column":235,"offset":7395},"indent":[]}}],"position":{"start":{"line":137,"column":1,"offset":7075},"end":{"line":138,"column":235,"offset":7395},"indent":[1]}},{"type":"paragraph","children":[{"type":"text","value":"The ","position":{"start":{"line":140,"column":1,"offset":7397},"end":{"line":140,"column":5,"offset":7401},"indent":[]}},{"type":"inlineCode","value":"ApplicationMaster","position":{"start":{"line":140,"column":5,"offset":7401},"end":{"line":140,"column":24,"offset":7420},"indent":[]}},{"type":"text","value":" has the responsibility of negotiating appropriate resource containers from the ","position":{"start":{"line":140,"column":24,"offset":7420},"end":{"line":140,"column":104,"offset":7500},"indent":[]}},{"type":"inlineCode","value":"Scheduler","position":{"start":{"line":140,"column":104,"offset":7500},"end":{"line":140,"column":115,"offset":7511},"indent":[]}},{"type":"text","value":", tracking their status and monitoring progress.","position":{"start":{"line":140,"column":115,"offset":7511},"end":{"line":140,"column":163,"offset":7559},"indent":[]}}],"position":{"start":{"line":140,"column":1,"offset":7397},"end":{"line":140,"column":163,"offset":7559},"indent":[]}},{"type":"heading","depth":3,"children":[{"type":"text","value":"Container","position":{"start":{"line":142,"column":5,"offset":7565},"end":{"line":142,"column":14,"offset":7574},"indent":[]}}],"position":{"start":{"line":142,"column":1,"offset":7561},"end":{"line":142,"column":14,"offset":7574},"indent":[]}},{"type":"paragraph","children":[{"type":"text","value":"Note that for YARN, a ","position":{"start":{"line":144,"column":1,"offset":7576},"end":{"line":144,"column":23,"offset":7598},"indent":[]}},{"type":"inlineCode","value":"container","position":{"start":{"line":144,"column":23,"offset":7598},"end":{"line":144,"column":34,"offset":7609},"indent":[]}},{"type":"text","value":" represents a collection of physical ","position":{"start":{"line":144,"column":34,"offset":7609},"end":{"line":144,"column":71,"offset":7646},"indent":[]}},{"type":"inlineCode","value":"resources","position":{"start":{"line":144,"column":71,"offset":7646},"end":{"line":144,"column":82,"offset":7657},"indent":[]}},{"type":"text","value":". Also could mean CPU cores, disk along with RAM.","position":{"start":{"line":144,"column":82,"offset":7657},"end":{"line":144,"column":131,"offset":7706},"indent":[]}}],"position":{"start":{"line":144,"column":1,"offset":7576},"end":{"line":144,"column":131,"offset":7706},"indent":[]}},{"type":"heading","depth":2,"children":[{"type":"text","value":"Hadoop Ecosystem","position":{"start":{"line":146,"column":4,"offset":7711},"end":{"line":146,"column":20,"offset":7727},"indent":[]}}],"position":{"start":{"line":146,"column":1,"offset":7708},"end":{"line":146,"column":20,"offset":7727},"indent":[]}},{"type":"paragraph","children":[{"type":"image","title":null,"url":"https://i.ytimg.com/vi/1WY63n2XRLM/maxresdefault.jpg","alt":"Ecosystem","position":{"start":{"line":148,"column":1,"offset":7729},"end":{"line":148,"column":67,"offset":7795},"indent":[]}}],"position":{"start":{"line":148,"column":1,"offset":7729},"end":{"line":148,"column":67,"offset":7795},"indent":[]}},{"type":"paragraph","children":[{"type":"text","value":"5 functions of Hadoop ecosystem:","position":{"start":{"line":150,"column":1,"offset":7797},"end":{"line":150,"column":33,"offset":7829},"indent":[]}}],"position":{"start":{"line":150,"column":1,"offset":7797},"end":{"line":150,"column":33,"offset":7829},"indent":[]}},{"type":"list","ordered":true,"start":1,"spread":false,"children":[{"type":"listItem","spread":false,"checked":null,"children":[{"type":"paragraph","children":[{"type":"text","value":"Data management using HDFS, HBase and YARN","position":{"start":{"line":152,"column":4,"offset":7834},"end":{"line":152,"column":46,"offset":7876},"indent":[]}}],"position":{"start":{"line":152,"column":4,"offset":7834},"end":{"line":152,"column":46,"offset":7876},"indent":[]}}],"position":{"start":{"line":152,"column":1,"offset":7831},"end":{"line":152,"column":46,"offset":7876},"indent":[]}},{"type":"listItem","spread":false,"checked":null,"children":[{"type":"paragraph","children":[{"type":"text","value":"Data access with MapReduce, Hive and Pig","position":{"start":{"line":153,"column":4,"offset":7880},"end":{"line":153,"column":44,"offset":7920},"indent":[]}}],"position":{"start":{"line":153,"column":4,"offset":7880},"end":{"line":153,"column":44,"offset":7920},"indent":[]}}],"position":{"start":{"line":153,"column":1,"offset":7877},"end":{"line":153,"column":44,"offset":7920},"indent":[]}},{"type":"listItem","spread":false,"checked":null,"children":[{"type":"paragraph","children":[{"type":"text","value":"Data ingestion and integration using Flume, Sqoop, Kafka and Storm","position":{"start":{"line":154,"column":4,"offset":7924},"end":{"line":154,"column":70,"offset":7990},"indent":[]}}],"position":{"start":{"line":154,"column":4,"offset":7924},"end":{"line":154,"column":70,"offset":7990},"indent":[]}}],"position":{"start":{"line":154,"column":1,"offset":7921},"end":{"line":154,"column":70,"offset":7990},"indent":[]}},{"type":"listItem","spread":false,"checked":null,"children":[{"type":"paragraph","children":[{"type":"text","value":"Data monitoring using Ambari, Zookeeper and Oozie","position":{"start":{"line":155,"column":4,"offset":7994},"end":{"line":155,"column":53,"offset":8043},"indent":[]}}],"position":{"start":{"line":155,"column":4,"offset":7994},"end":{"line":155,"column":53,"offset":8043},"indent":[]}}],"position":{"start":{"line":155,"column":1,"offset":7991},"end":{"line":155,"column":53,"offset":8043},"indent":[]}},{"type":"listItem","spread":false,"checked":null,"children":[{"type":"paragraph","children":[{"type":"text","value":"Data governance and security using Falcon, Ranger and Knox","position":{"start":{"line":156,"column":4,"offset":8047},"end":{"line":156,"column":62,"offset":8105},"indent":[]}}],"position":{"start":{"line":156,"column":4,"offset":8047},"end":{"line":156,"column":62,"offset":8105},"indent":[]}}],"position":{"start":{"line":156,"column":1,"offset":8044},"end":{"line":156,"column":62,"offset":8105},"indent":[]}}],"position":{"start":{"line":152,"column":1,"offset":7831},"end":{"line":156,"column":62,"offset":8105},"indent":[1,1,1,1]}},{"type":"export","value":"export const _frontmatter = {}","position":{"start":{"line":159,"column":1,"offset":8108},"end":{"line":159,"column":31,"offset":8138},"indent":[]}}],"position":{"start":{"line":1,"column":1,"offset":0},"end":{"line":159,"column":31,"offset":8138}}},"scopeImports":[],"scopeIdentifiers":[],"rawMDXOutput":"/* @jsx mdx */\nimport { mdx } from '@mdx-js/react';\n/* @jsx mdx */\nimport DefaultLayout from \"/Users/dennis.okeeffe/Project-Imposter/developer-notes/node_modules/gatsby-theme-docz/src/base/Layout.js\"\nexport const _frontmatter = {};\nconst makeShortcode = name => function MDXDefaultShortcode(props) {\n  console.warn(\"Component \" + name + \" was not imported, exported, or provided by MDXProvider as global scope\")\n  return <div {...props}/>\n};\n\nconst layoutProps = {\n  _frontmatter\n};\nconst MDXLayout = DefaultLayout\nexport default function MDXContent({\n  components,\n  ...props\n}) {\n  return <MDXLayout {...layoutProps} {...props} components={components} mdxType=\"MDXLayout\">\n\n\n    <h1 {...{\n      \"id\": \"big-data-tools-and-technologies\"\n    }}>{`Big Data Tools and Technologies`}</h1>\n    <h2 {...{\n      \"id\": \"apache-hadoop\"\n    }}>{`Apache Hadoop`}</h2>\n    <p>{`Hadoop is a very significant player in the Big Data landscape.`}</p>\n    <p>{`It's an open-sourced framework for distributed storage and processing of very large data sets.`}</p>\n    <p>{`Originally built in 2005 by a Yahoo engineer.`}</p>\n    <p>{`It was inspired by Google's `}<inlineCode parentName=\"p\">{`MapReduce`}</inlineCode>{` and the `}<inlineCode parentName=\"p\">{`Google File System`}</inlineCode>{` papers.`}</p>\n    <p>{`It was written in Java to implement the `}<inlineCode parentName=\"p\">{`MapReduce`}</inlineCode>{` programming model for scalable, reliable and distributed computing.`}</p>\n    <p>{`The framework is composed of:`}</p>\n    <ol>\n      <li parentName=\"ol\">{`Hadoop Common: Contains libraries and utilities needed by other Hadoop modules.`}</li>\n      <li parentName=\"ol\">{`Hadoop Distributed File System (HDFS): A distributed file system that stores data on the commodity machines, providing very high aggregate bandwidth across the cluster.`}</li>\n      <li parentName=\"ol\">{`Hadoop MapReduce: A programming model for large-scale data processing.`}</li>\n      <li parentName=\"ol\">{`Hadoop YARN: A resource management platform responsible for managing compute resources in clusters and using them for the scheduling of users' applications.`}</li>\n    </ol>\n    <h2 {...{\n      \"id\": \"hdfs\"\n    }}>{`HDFS`}</h2>\n    <ul>\n      <li parentName=\"ul\">{`Structured like a regular Unix file system with data storage distributed across several machines in the cluster.`}</li>\n      <li parentName=\"ul\">{`Data service that sits atop regular file systems, allowing a fault tolerant, resilient clustered approach to storing and processing data.`}</li>\n      <li parentName=\"ul\">{`Fault-tolerant: Detection of faults and quick automatic recovery is a core architectural goal.`}</li>\n      <li parentName=\"ul\">{`Tuned to support large files. Typically a file is GB or TB and can support tens of millions of files by scaling to hundreds of nodes in a cluster.`}</li>\n      <li parentName=\"ul\">{`Follows `}<inlineCode parentName=\"li\">{`write once, read multiple`}</inlineCode>{` approach, simplifying data coherency issues and enabling high throughput data access. Example is a web crawler application.`}</li>\n      <li parentName=\"ul\">{`Optimized for throughput rather than latency. This makes it suited for long batch operations on large scale data rather than interactive analysis on streaming data.`}</li>\n      <li parentName=\"ul\">{`Moving computation near the data reduces network congestion and increses throughput. HDFS provides interfaces or applications to move closer to data storage.`}</li>\n    </ul>\n    <h3 {...{\n      \"id\": \"architecture\"\n    }}>{`Architecture`}</h3>\n    <ol>\n      <li parentName=\"ol\">{`Leader-follower architecture, where `}<inlineCode parentName=\"li\">{`Namenode`}</inlineCode>{` is the leader and `}<inlineCode parentName=\"li\">{`Datanodes`}</inlineCode>{` are slaves.`}</li>\n      <li parentName=\"ol\">{`Files split into blocks, and blocks are stored on datanodes (generally one per node within cluster).`}</li>\n      <li parentName=\"ol\">{`Datanodes manage storage attached to nodes that they run on.`}</li>\n      <li parentName=\"ol\">{`Namenode controls all metadata, including what blocks make up a file and which datanode the blocks are stored on.`}</li>\n      <li parentName=\"ol\">{`Namenode executres file system operations like opening, closing and renaming files and directories.`}</li>\n      <li parentName=\"ol\">{`Datanodes serve read and write requests from the clients.`}</li>\n      <li parentName=\"ol\">{`Datanodes perform block creation, deletion, replication upon instruction from the Namenode.`}</li>\n      <li parentName=\"ol\">{`Namenode and Datanode are Java software designed to run on commodity hardware that supports Java.`}</li>\n      <li parentName=\"ol\">{`Usually a cluster contains a single Namenode and multiple datanodes, one each for each node in the cluster.`}</li>\n    </ol>\n    <p><img alt=\"HDFS Architecture\" src=\"https://hadoop.apache.org/docs/r1.2.1/images/hdfsarchitecture.gif\" /></p>\n    <p>{`The `}<inlineCode parentName=\"p\">{`Namenode`}</inlineCode>{` makes all decisions around replication of blocks for data durability. Periodically receives heartbeat and `}<inlineCode parentName=\"p\">{`BlockReport`}</inlineCode>{` from datanodes in the cluster. Receipt of heartbeat is the health check.`}</p>\n    <h2 {...{\n      \"id\": \"hadoop-mapreduce\"\n    }}>{`Hadoop MapReduce`}</h2>\n    <p>{`A framework that makes it easy to write applications which can consume huge amouts of data.`}</p>\n    <p>{`It allows processing in parallel on large clusters consisting of thousands of nodes in a manner that is reliable and fault tolerant.`}</p>\n    <p>{`The MapReduce layer consists of:`}</p>\n    <ol>\n      <li parentName=\"ol\">{`MapReduce Java API to write workflows`}</li>\n      <li parentName=\"ol\">{`Services to manage these workflows and provide the scheduling, distribution and parallelizing.`}</li>\n    </ol>\n    <h3 {...{\n      \"id\": \"mapreduce-jobs\"\n    }}>{`MapReduce jobs`}</h3>\n    <ol>\n      <li parentName=\"ol\">{`Splits the data sets into independent chunks.`}</li>\n      <li parentName=\"ol\">{`Data sets are processed by map tasks in parallel.`}</li>\n      <li parentName=\"ol\">{`MapReduce sorts the output of map jobs and feeds them to reduce tasks.`}</li>\n      <li parentName=\"ol\">{`Both input and output of map and reduce are stored on the file system.`}</li>\n      <li parentName=\"ol\">{`Framework takes care of scheduling tasks, monitoring them and re-executing failed tasks.`}</li>\n      <li parentName=\"ol\">{`MapReduce framework and HDFS are running on the same set of nodes. Tasks are scheduled on nodes where data is already present, hence yielding high bandwidth across the cluster.`}</li>\n    </ol>\n    <h3 {...{\n      \"id\": \"inputs-and-outputs-of-a-mapreduce-job\"\n    }}>{`Inputs and Outputs of a MapReduce Job`}</h3>\n    <ul>\n      <li parentName=\"ul\">{`Exclusively operates on key-value pairs.`}</li>\n      <li parentName=\"ul\">{`Input is large scale data set which benefits from parallel processing and does not fit on a single machine.`}</li>\n      <li parentName=\"ul\">{`Input split into independent data sets and map function produces key-value pair for each record in the data set.`}</li>\n      <li parentName=\"ul\">{`Output of mappers is shuffled, sorted, grouped and passed to the reducers.`}</li>\n      <li parentName=\"ul\">{`Reducer function applied to sets of key-value pairs that share the same key. The reducer function often agregates the value for the pairs with the same key.`}</li>\n    </ul>\n    <p>{`It is important to know that:`}</p>\n    <ol>\n      <li parentName=\"ol\">{`Almost all data can be mapped to a key-value pair using a map function.`}</li>\n      <li parentName=\"ol\">{`Keys and values can be of any type. If using a custom type, the type must be implement a writable interface.`}</li>\n      <li parentName=\"ol\"><inlineCode parentName=\"li\">{`MapReduce`}</inlineCode>{` cannot be used if a computation of a value depends on a previously computed value. Recursive funcs like Fibonnaci cannot be implemented using `}<inlineCode parentName=\"li\">{`MapReduce`}</inlineCode>{`.`}</li>\n    </ol>\n    <p>{`This is an example of a word count `}<inlineCode parentName=\"p\">{`MapReduce`}</inlineCode>{` job.`}</p>\n    <p><img alt=\"Example of MapReduce job\" src=\"https://www.guru99.com/images/Big_Data/061114_0930_Introductio1.png\" /></p>\n    <p>{`The order of a job goes as the following:`}</p>\n    <ol>\n      <li parentName=\"ol\">{`Input`}</li>\n      <li parentName=\"ol\">{`Splitting`}</li>\n      <li parentName=\"ol\">{`Mapping`}</li>\n      <li parentName=\"ol\">{`Shuffle`}</li>\n      <li parentName=\"ol\">{`Reduce`}</li>\n      <li parentName=\"ol\">{`Final Result`}</li>\n    </ol>\n    <h3 {...{\n      \"id\": \"example-applications-of-mapreduce\"\n    }}>{`Example Applications of MapReduce`}</h3>\n    <ol>\n      <li parentName=\"ol\">{`Counting votes by processing data from each polling booth.`}</li>\n      <li parentName=\"ol\">{`Aggregating electricy consumption from data points collected across a large geographical area.`}</li>\n      <li parentName=\"ol\">{`Used by Google Maps to calculate nearest neighbour.`}</li>\n      <li parentName=\"ol\">{`Performing statistical aggregate type functions on large data sets.`}</li>\n      <li parentName=\"ol\">{`Counting number of href links in web log files for clickstream analysis.`}</li>\n    </ol>\n    <h3 {...{\n      \"id\": \"writing-and-running-hadoop-mapreduce-jobs\"\n    }}>{`Writing and Running Hadoop MapReduce Jobs`}</h3>\n    <p>{`Typicall jobs are written in Java, but can also be written using:`}</p>\n    <ol>\n      <li parentName=\"ol\">{`Hadoop Streaming: A utility which allows users to create an run MapReduce jobs with any executables.`}</li>\n      <li parentName=\"ol\">{`Hadoop Pipes: C++ API to implement MapReduce applications`}</li>\n    </ol>\n    <h3 {...{\n      \"id\": \"hadoop-job-configurations\"\n    }}>{`Hadoop Job configurations`}</h3>\n    <p>{`Consists of:`}</p>\n    <ul>\n      <li parentName=\"ul\">{`Input and output locations on HDFS.`}</li>\n      <li parentName=\"ul\">{`Map and reduce functions via implementations of interfaces or abstract classes.`}</li>\n      <li parentName=\"ul\">{`Other job parameters.`}</li>\n    </ul>\n    <p>{`A Hadoop job client submits the job (jar/executable) and configuration to the `}<inlineCode parentName=\"p\">{`ResourceManager`}</inlineCode>{` in `}<inlineCode parentName=\"p\">{`YARN`}</inlineCode>{` which distributes them to the workers and performs functions like scheduling, monitoring and providing status and diagnostic information.`}</p>\n    <h2 {...{\n      \"id\": \"yet-another-resource-negotiator-yarn\"\n    }}>{`Yet Another Resource Negotiator (YARN)`}</h2>\n    <p>{`Introduced in Hadoop 2.0, YARN provides a general processing platform not constrained to `}<inlineCode parentName=\"p\">{`MapReduce`}</inlineCode>{`.`}</p>\n    <p><inlineCode parentName=\"p\">{`Global ResourceManager`}</inlineCode>{` is the authority that delegates resources among the applications in the system.`}</p>\n    <p>{`It has a `}<inlineCode parentName=\"p\">{`NodeManager`}</inlineCode>{` on each node that is responsible for containers, monitoring their resource usage (CPU, memory, disk, network) and reporting this data to the `}<inlineCode parentName=\"p\">{`ResourceManager`}</inlineCode>{`.`}</p>\n    <p>{`The `}<inlineCode parentName=\"p\">{`ResourceManager`}</inlineCode>{` has two components:`}</p>\n    <ol>\n      <li parentName=\"ol\">{`Scheduler - responsible for allocating resources the various running applications.`}</li>\n      <li parentName=\"ol\">{`ApplicationsManager - responsible for accepting job-submissions, negotiating the first container for executing the application-specific `}<inlineCode parentName=\"li\">{`ApplicationMaster`}</inlineCode>{` and provides the service for restarting the `}<inlineCode parentName=\"li\">{`ApplicationMaster`}</inlineCode>{` on failure.`}</li>\n    </ol>\n    <p>{`The `}<inlineCode parentName=\"p\">{`ApplicationMaster`}</inlineCode>{` has the responsibility of negotiating appropriate resource containers from the `}<inlineCode parentName=\"p\">{`Scheduler`}</inlineCode>{`, tracking their status and monitoring progress.`}</p>\n    <h3 {...{\n      \"id\": \"container\"\n    }}>{`Container`}</h3>\n    <p>{`Note that for YARN, a `}<inlineCode parentName=\"p\">{`container`}</inlineCode>{` represents a collection of physical `}<inlineCode parentName=\"p\">{`resources`}</inlineCode>{`. Also could mean CPU cores, disk along with RAM.`}</p>\n    <h2 {...{\n      \"id\": \"hadoop-ecosystem\"\n    }}>{`Hadoop Ecosystem`}</h2>\n    <p><img alt=\"Ecosystem\" src=\"https://i.ytimg.com/vi/1WY63n2XRLM/maxresdefault.jpg\" /></p>\n    <p>{`5 functions of Hadoop ecosystem:`}</p>\n    <ol>\n      <li parentName=\"ol\">{`Data management using HDFS, HBase and YARN`}</li>\n      <li parentName=\"ol\">{`Data access with MapReduce, Hive and Pig`}</li>\n      <li parentName=\"ol\">{`Data ingestion and integration using Flume, Sqoop, Kafka and Storm`}</li>\n      <li parentName=\"ol\">{`Data monitoring using Ambari, Zookeeper and Oozie`}</li>\n      <li parentName=\"ol\">{`Data governance and security using Falcon, Ranger and Knox`}</li>\n    </ol>\n\n    </MDXLayout>;\n}\n\n;\nMDXContent.isMDXComponent = true;"}}